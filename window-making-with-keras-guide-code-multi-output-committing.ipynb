{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from riotwatcher import LolWatcher, ApiError\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from time import sleep\n",
    "\n",
    "with open('data/api-key.txt', 'r') as api:\n",
    "    API_KEY = api.read()\n",
    "lol_watcher = LolWatcher(API_KEY)\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4)\"+\n",
    "                    \" AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.135\"+\n",
    "                    \" Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Accept-Charset\": \"application/x-www-form-urlencoded; charset=UTF-8\",\n",
    "    \"Origin\": \"https://developer.riotgames.com\",\n",
    "    \"X-Riot-Token\": API_KEY\n",
    "}\n",
    "\n",
    "CHAMP_ID = 64\n",
    "TESTMATCH = \"4748107995\"\n",
    "\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "#import IPython\n",
    "#import IPython.display\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_dict = pickle.load(open(\"data/timeline-di.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "gameids = list(dfs_dict.keys())\n",
    "gameids.sort()\n",
    "train_ids = gameids[0:int(len(gameids)*.8)]\n",
    "val_ids = gameids[int(len(gameids)*.8):int(len(gameids)*.9)]\n",
    "test_ids = gameids[int(len(gameids)*.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>0currentGold</th>\n",
       "      <th>1currentGold</th>\n",
       "      <th>2currentGold</th>\n",
       "      <th>3currentGold</th>\n",
       "      <th>4currentGold</th>\n",
       "      <th>5currentGold</th>\n",
       "      <th>6currentGold</th>\n",
       "      <th>7currentGold</th>\n",
       "      <th>8currentGold</th>\n",
       "      <th>...</th>\n",
       "      <th>1y</th>\n",
       "      <th>2y</th>\n",
       "      <th>3y</th>\n",
       "      <th>4y</th>\n",
       "      <th>6y</th>\n",
       "      <th>7y</th>\n",
       "      <th>8y</th>\n",
       "      <th>9y</th>\n",
       "      <th>player_x</th>\n",
       "      <th>player_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>361.0</td>\n",
       "      <td>293.0</td>\n",
       "      <td>471.0</td>\n",
       "      <td>649.0</td>\n",
       "      <td>14291.0</td>\n",
       "      <td>14223.0</td>\n",
       "      <td>14401.0</td>\n",
       "      <td>14579.0</td>\n",
       "      <td>14486.0</td>\n",
       "      <td>14511.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60021.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5918.0</td>\n",
       "      <td>7122.0</td>\n",
       "      <td>1061.0</td>\n",
       "      <td>5697.0</td>\n",
       "      <td>6837.0</td>\n",
       "      <td>6371.0</td>\n",
       "      <td>4256.0</td>\n",
       "      <td>5165.0</td>\n",
       "      <td>13192.0</td>\n",
       "      <td>14166.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>120038.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2128.0</td>\n",
       "      <td>6876.0</td>\n",
       "      <td>2722.0</td>\n",
       "      <td>2688.0</td>\n",
       "      <td>7753.0</td>\n",
       "      <td>7563.0</td>\n",
       "      <td>3038.0</td>\n",
       "      <td>2617.0</td>\n",
       "      <td>4234.0</td>\n",
       "      <td>13602.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>180049.0</td>\n",
       "      <td>455.0</td>\n",
       "      <td>453.0</td>\n",
       "      <td>469.0</td>\n",
       "      <td>614.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>381.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5054.0</td>\n",
       "      <td>6181.0</td>\n",
       "      <td>4045.0</td>\n",
       "      <td>3444.0</td>\n",
       "      <td>5250.0</td>\n",
       "      <td>7136.0</td>\n",
       "      <td>5056.0</td>\n",
       "      <td>7300.0</td>\n",
       "      <td>3871.0</td>\n",
       "      <td>13475.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>240068.0</td>\n",
       "      <td>781.0</td>\n",
       "      <td>947.0</td>\n",
       "      <td>717.0</td>\n",
       "      <td>1358.0</td>\n",
       "      <td>339.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5411.0</td>\n",
       "      <td>7268.0</td>\n",
       "      <td>2113.0</td>\n",
       "      <td>2290.0</td>\n",
       "      <td>13996.0</td>\n",
       "      <td>8934.0</td>\n",
       "      <td>8532.0</td>\n",
       "      <td>2420.0</td>\n",
       "      <td>4153.0</td>\n",
       "      <td>13654.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>300080.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>1232.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>1830.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>526.0</td>\n",
       "      <td>495.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8268.0</td>\n",
       "      <td>6871.0</td>\n",
       "      <td>1171.0</td>\n",
       "      <td>1372.0</td>\n",
       "      <td>9862.0</td>\n",
       "      <td>7741.0</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>1687.0</td>\n",
       "      <td>4254.0</td>\n",
       "      <td>13656.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>360102.0</td>\n",
       "      <td>418.0</td>\n",
       "      <td>1314.0</td>\n",
       "      <td>587.0</td>\n",
       "      <td>965.0</td>\n",
       "      <td>793.0</td>\n",
       "      <td>728.0</td>\n",
       "      <td>821.0</td>\n",
       "      <td>859.0</td>\n",
       "      <td>533.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13841.0</td>\n",
       "      <td>13390.0</td>\n",
       "      <td>520.0</td>\n",
       "      <td>1237.0</td>\n",
       "      <td>4689.0</td>\n",
       "      <td>5565.0</td>\n",
       "      <td>1573.0</td>\n",
       "      <td>1547.0</td>\n",
       "      <td>4405.0</td>\n",
       "      <td>14040.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>420109.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>346.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>331.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>492.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3688.0</td>\n",
       "      <td>7655.0</td>\n",
       "      <td>2539.0</td>\n",
       "      <td>2698.0</td>\n",
       "      <td>13300.0</td>\n",
       "      <td>11346.0</td>\n",
       "      <td>2695.0</td>\n",
       "      <td>3577.0</td>\n",
       "      <td>3241.0</td>\n",
       "      <td>13286.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>480142.0</td>\n",
       "      <td>383.0</td>\n",
       "      <td>802.0</td>\n",
       "      <td>688.0</td>\n",
       "      <td>840.0</td>\n",
       "      <td>430.0</td>\n",
       "      <td>921.0</td>\n",
       "      <td>482.0</td>\n",
       "      <td>429.0</td>\n",
       "      <td>344.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5488.0</td>\n",
       "      <td>5054.0</td>\n",
       "      <td>4249.0</td>\n",
       "      <td>5181.0</td>\n",
       "      <td>10001.0</td>\n",
       "      <td>7477.0</td>\n",
       "      <td>5826.0</td>\n",
       "      <td>5571.0</td>\n",
       "      <td>2137.0</td>\n",
       "      <td>11848.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>540170.0</td>\n",
       "      <td>781.0</td>\n",
       "      <td>1284.0</td>\n",
       "      <td>961.0</td>\n",
       "      <td>1409.0</td>\n",
       "      <td>707.0</td>\n",
       "      <td>1177.0</td>\n",
       "      <td>643.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>545.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7369.0</td>\n",
       "      <td>6941.0</td>\n",
       "      <td>4785.0</td>\n",
       "      <td>7242.0</td>\n",
       "      <td>7701.0</td>\n",
       "      <td>8166.0</td>\n",
       "      <td>7717.0</td>\n",
       "      <td>7742.0</td>\n",
       "      <td>4345.0</td>\n",
       "      <td>13525.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp  0currentGold  1currentGold  2currentGold  3currentGold  \\\n",
       "0        0.0         500.0         500.0         500.0         500.0   \n",
       "1    60021.0           0.0           0.0           0.0           0.0   \n",
       "2   120038.0         126.0         194.0          98.0          63.0   \n",
       "3   180049.0         455.0         453.0         469.0         614.0   \n",
       "4   240068.0         781.0         947.0         717.0        1358.0   \n",
       "5   300080.0          87.0        1232.0         176.0        1830.0   \n",
       "6   360102.0         418.0        1314.0         587.0         965.0   \n",
       "7   420109.0          59.0         346.0         257.0         331.0   \n",
       "8   480142.0         383.0         802.0         688.0         840.0   \n",
       "9   540170.0         781.0        1284.0         961.0        1409.0   \n",
       "\n",
       "   4currentGold  5currentGold  6currentGold  7currentGold  8currentGold  ...  \\\n",
       "0         500.0         500.0         500.0         500.0         500.0  ...   \n",
       "1           0.0           0.0           0.0           0.0          15.0  ...   \n",
       "2          44.0          42.0         121.0          98.0          98.0  ...   \n",
       "3         134.0         311.0         138.0         381.0         171.0  ...   \n",
       "4         339.0         560.0         261.0          79.0          46.0  ...   \n",
       "5         558.0         144.0         526.0         495.0         297.0  ...   \n",
       "6         793.0         728.0         821.0         859.0         533.0  ...   \n",
       "7         192.0         492.0          48.0          49.0          83.0  ...   \n",
       "8         430.0         921.0         482.0         429.0         344.0  ...   \n",
       "9         707.0        1177.0         643.0          55.0         545.0  ...   \n",
       "\n",
       "        1y       2y      3y      4y       6y       7y       8y       9y  \\\n",
       "0    361.0    293.0   471.0   649.0  14291.0  14223.0  14401.0  14579.0   \n",
       "1   5918.0   7122.0  1061.0  5697.0   6837.0   6371.0   4256.0   5165.0   \n",
       "2   2128.0   6876.0  2722.0  2688.0   7753.0   7563.0   3038.0   2617.0   \n",
       "3   5054.0   6181.0  4045.0  3444.0   5250.0   7136.0   5056.0   7300.0   \n",
       "4   5411.0   7268.0  2113.0  2290.0  13996.0   8934.0   8532.0   2420.0   \n",
       "5   8268.0   6871.0  1171.0  1372.0   9862.0   7741.0   1260.0   1687.0   \n",
       "6  13841.0  13390.0   520.0  1237.0   4689.0   5565.0   1573.0   1547.0   \n",
       "7   3688.0   7655.0  2539.0  2698.0  13300.0  11346.0   2695.0   3577.0   \n",
       "8   5488.0   5054.0  4249.0  5181.0  10001.0   7477.0   5826.0   5571.0   \n",
       "9   7369.0   6941.0  4785.0  7242.0   7701.0   8166.0   7717.0   7742.0   \n",
       "\n",
       "   player_x  player_y  \n",
       "0   14486.0   14511.0  \n",
       "1   13192.0   14166.0  \n",
       "2    4234.0   13602.0  \n",
       "3    3871.0   13475.0  \n",
       "4    4153.0   13654.0  \n",
       "5    4254.0   13656.0  \n",
       "6    4405.0   14040.0  \n",
       "7    3241.0   13286.0  \n",
       "8    2137.0   11848.0  \n",
       "9    4345.0   13525.0  \n",
       "\n",
       "[10 rows x 61 columns]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_dict[train_ids[1]][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_ids[0]\n",
    "test0 = dfs_dict[train_ids[0]][0:20].drop([\"timestamp\"],axis=1)\n",
    "test1 = dfs_dict[train_ids[1]][0:20].drop([\"timestamp\"],axis=1)\n",
    "column_indices = {name: i for i, name in enumerate(test0.columns)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0currentGold</th>\n",
       "      <th>1currentGold</th>\n",
       "      <th>2currentGold</th>\n",
       "      <th>3currentGold</th>\n",
       "      <th>4currentGold</th>\n",
       "      <th>5currentGold</th>\n",
       "      <th>6currentGold</th>\n",
       "      <th>7currentGold</th>\n",
       "      <th>8currentGold</th>\n",
       "      <th>9currentGold</th>\n",
       "      <th>...</th>\n",
       "      <th>1y</th>\n",
       "      <th>3y</th>\n",
       "      <th>4y</th>\n",
       "      <th>5y</th>\n",
       "      <th>6y</th>\n",
       "      <th>7y</th>\n",
       "      <th>8y</th>\n",
       "      <th>9y</th>\n",
       "      <th>player_x</th>\n",
       "      <th>player_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>361.0</td>\n",
       "      <td>471.0</td>\n",
       "      <td>649.0</td>\n",
       "      <td>14511.0</td>\n",
       "      <td>14291.0</td>\n",
       "      <td>14223.0</td>\n",
       "      <td>14401.0</td>\n",
       "      <td>14579.0</td>\n",
       "      <td>351.0</td>\n",
       "      <td>293.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>542.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>679.0</td>\n",
       "      <td>13885.0</td>\n",
       "      <td>11463.0</td>\n",
       "      <td>8416.0</td>\n",
       "      <td>7493.0</td>\n",
       "      <td>11457.0</td>\n",
       "      <td>6348.0</td>\n",
       "      <td>9817.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>122.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8042.0</td>\n",
       "      <td>2285.0</td>\n",
       "      <td>2107.0</td>\n",
       "      <td>12344.0</td>\n",
       "      <td>10103.0</td>\n",
       "      <td>7015.0</td>\n",
       "      <td>3101.0</td>\n",
       "      <td>2597.0</td>\n",
       "      <td>6598.0</td>\n",
       "      <td>6970.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>356.0</td>\n",
       "      <td>634.0</td>\n",
       "      <td>486.0</td>\n",
       "      <td>521.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>359.0</td>\n",
       "      <td>540.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>373.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5011.0</td>\n",
       "      <td>3118.0</td>\n",
       "      <td>3266.0</td>\n",
       "      <td>13855.0</td>\n",
       "      <td>4991.0</td>\n",
       "      <td>7337.0</td>\n",
       "      <td>3991.0</td>\n",
       "      <td>4151.0</td>\n",
       "      <td>7235.0</td>\n",
       "      <td>6689.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>128.0</td>\n",
       "      <td>1475.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>485.0</td>\n",
       "      <td>816.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>648.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3263.0</td>\n",
       "      <td>2921.0</td>\n",
       "      <td>2816.0</td>\n",
       "      <td>10840.0</td>\n",
       "      <td>9761.0</td>\n",
       "      <td>9040.0</td>\n",
       "      <td>3937.0</td>\n",
       "      <td>11836.0</td>\n",
       "      <td>7473.0</td>\n",
       "      <td>7689.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>453.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>454.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>413.0</td>\n",
       "      <td>379.0</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7169.0</td>\n",
       "      <td>1556.0</td>\n",
       "      <td>1712.0</td>\n",
       "      <td>12890.0</td>\n",
       "      <td>8567.0</td>\n",
       "      <td>8337.0</td>\n",
       "      <td>3373.0</td>\n",
       "      <td>3711.0</td>\n",
       "      <td>8044.0</td>\n",
       "      <td>7598.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>803.0</td>\n",
       "      <td>630.0</td>\n",
       "      <td>751.0</td>\n",
       "      <td>439.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>527.0</td>\n",
       "      <td>696.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>429.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12602.0</td>\n",
       "      <td>1340.0</td>\n",
       "      <td>4032.0</td>\n",
       "      <td>13411.0</td>\n",
       "      <td>4964.0</td>\n",
       "      <td>7590.0</td>\n",
       "      <td>2309.0</td>\n",
       "      <td>5108.0</td>\n",
       "      <td>6946.0</td>\n",
       "      <td>7036.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1316.0</td>\n",
       "      <td>1366.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>534.0</td>\n",
       "      <td>817.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5325.0</td>\n",
       "      <td>554.0</td>\n",
       "      <td>5437.0</td>\n",
       "      <td>12869.0</td>\n",
       "      <td>11326.0</td>\n",
       "      <td>8390.0</td>\n",
       "      <td>4371.0</td>\n",
       "      <td>12836.0</td>\n",
       "      <td>2871.0</td>\n",
       "      <td>2268.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>89.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>361.0</td>\n",
       "      <td>323.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>308.0</td>\n",
       "      <td>946.0</td>\n",
       "      <td>1129.0</td>\n",
       "      <td>674.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10843.0</td>\n",
       "      <td>2752.0</td>\n",
       "      <td>5456.0</td>\n",
       "      <td>8475.0</td>\n",
       "      <td>10091.0</td>\n",
       "      <td>9822.0</td>\n",
       "      <td>5626.0</td>\n",
       "      <td>9909.0</td>\n",
       "      <td>7742.0</td>\n",
       "      <td>7255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>367.0</td>\n",
       "      <td>395.0</td>\n",
       "      <td>1114.0</td>\n",
       "      <td>968.0</td>\n",
       "      <td>607.0</td>\n",
       "      <td>557.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>954.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2446.0</td>\n",
       "      <td>3844.0</td>\n",
       "      <td>3924.0</td>\n",
       "      <td>13650.0</td>\n",
       "      <td>8741.0</td>\n",
       "      <td>13681.0</td>\n",
       "      <td>4822.0</td>\n",
       "      <td>4766.0</td>\n",
       "      <td>8023.0</td>\n",
       "      <td>7885.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>988.0</td>\n",
       "      <td>889.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>1380.0</td>\n",
       "      <td>836.0</td>\n",
       "      <td>905.0</td>\n",
       "      <td>648.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>1307.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>...</td>\n",
       "      <td>11534.0</td>\n",
       "      <td>4371.0</td>\n",
       "      <td>4248.0</td>\n",
       "      <td>9874.0</td>\n",
       "      <td>9630.0</td>\n",
       "      <td>9898.0</td>\n",
       "      <td>5467.0</td>\n",
       "      <td>6585.0</td>\n",
       "      <td>1252.0</td>\n",
       "      <td>1886.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>554.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>638.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>372.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>632.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8858.0</td>\n",
       "      <td>1565.0</td>\n",
       "      <td>4859.0</td>\n",
       "      <td>11640.0</td>\n",
       "      <td>6555.0</td>\n",
       "      <td>7502.0</td>\n",
       "      <td>3013.0</td>\n",
       "      <td>3186.0</td>\n",
       "      <td>7075.0</td>\n",
       "      <td>6922.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1503.0</td>\n",
       "      <td>655.0</td>\n",
       "      <td>927.0</td>\n",
       "      <td>624.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>379.0</td>\n",
       "      <td>815.0</td>\n",
       "      <td>944.0</td>\n",
       "      <td>413.0</td>\n",
       "      <td>826.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6039.0</td>\n",
       "      <td>3058.0</td>\n",
       "      <td>3066.0</td>\n",
       "      <td>13843.0</td>\n",
       "      <td>4702.0</td>\n",
       "      <td>6947.0</td>\n",
       "      <td>3803.0</td>\n",
       "      <td>5725.0</td>\n",
       "      <td>6208.0</td>\n",
       "      <td>6108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>535.0</td>\n",
       "      <td>1321.0</td>\n",
       "      <td>331.0</td>\n",
       "      <td>893.0</td>\n",
       "      <td>425.0</td>\n",
       "      <td>648.0</td>\n",
       "      <td>1585.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>881.0</td>\n",
       "      <td>1025.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13541.0</td>\n",
       "      <td>3039.0</td>\n",
       "      <td>3873.0</td>\n",
       "      <td>14048.0</td>\n",
       "      <td>13329.0</td>\n",
       "      <td>8733.0</td>\n",
       "      <td>5793.0</td>\n",
       "      <td>4492.0</td>\n",
       "      <td>7816.0</td>\n",
       "      <td>7999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1279.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>538.0</td>\n",
       "      <td>1742.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>771.0</td>\n",
       "      <td>2098.0</td>\n",
       "      <td>1083.0</td>\n",
       "      <td>1359.0</td>\n",
       "      <td>1436.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7701.0</td>\n",
       "      <td>5947.0</td>\n",
       "      <td>7509.0</td>\n",
       "      <td>12368.0</td>\n",
       "      <td>8283.0</td>\n",
       "      <td>8230.0</td>\n",
       "      <td>7160.0</td>\n",
       "      <td>9050.0</td>\n",
       "      <td>1256.0</td>\n",
       "      <td>1095.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>717.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>474.0</td>\n",
       "      <td>1517.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>652.0</td>\n",
       "      <td>815.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3963.0</td>\n",
       "      <td>7280.0</td>\n",
       "      <td>482.0</td>\n",
       "      <td>8152.0</td>\n",
       "      <td>10842.0</td>\n",
       "      <td>13602.0</td>\n",
       "      <td>8196.0</td>\n",
       "      <td>8667.0</td>\n",
       "      <td>7133.0</td>\n",
       "      <td>7148.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1188.0</td>\n",
       "      <td>1057.0</td>\n",
       "      <td>1402.0</td>\n",
       "      <td>699.0</td>\n",
       "      <td>464.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>510.0</td>\n",
       "      <td>931.0</td>\n",
       "      <td>1243.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6598.0</td>\n",
       "      <td>7373.0</td>\n",
       "      <td>7847.0</td>\n",
       "      <td>11955.0</td>\n",
       "      <td>10590.0</td>\n",
       "      <td>8602.0</td>\n",
       "      <td>9626.0</td>\n",
       "      <td>9397.0</td>\n",
       "      <td>9254.0</td>\n",
       "      <td>7471.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>502.0</td>\n",
       "      <td>1327.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>634.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>1509.0</td>\n",
       "      <td>639.0</td>\n",
       "      <td>1722.0</td>\n",
       "      <td>694.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6385.0</td>\n",
       "      <td>6863.0</td>\n",
       "      <td>1107.0</td>\n",
       "      <td>14025.0</td>\n",
       "      <td>10583.0</td>\n",
       "      <td>11714.0</td>\n",
       "      <td>7330.0</td>\n",
       "      <td>7373.0</td>\n",
       "      <td>2815.0</td>\n",
       "      <td>2201.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>625.0</td>\n",
       "      <td>1579.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>584.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>1109.0</td>\n",
       "      <td>2497.0</td>\n",
       "      <td>851.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5060.0</td>\n",
       "      <td>12398.0</td>\n",
       "      <td>6453.0</td>\n",
       "      <td>6463.0</td>\n",
       "      <td>6437.0</td>\n",
       "      <td>6681.0</td>\n",
       "      <td>7231.0</td>\n",
       "      <td>6545.0</td>\n",
       "      <td>7628.0</td>\n",
       "      <td>6715.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1020.0</td>\n",
       "      <td>1803.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>1412.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2805.0</td>\n",
       "      <td>1123.0</td>\n",
       "      <td>457.0</td>\n",
       "      <td>519.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8438.0</td>\n",
       "      <td>13454.0</td>\n",
       "      <td>2322.0</td>\n",
       "      <td>14074.0</td>\n",
       "      <td>1422.0</td>\n",
       "      <td>1340.0</td>\n",
       "      <td>8120.0</td>\n",
       "      <td>1559.0</td>\n",
       "      <td>5584.0</td>\n",
       "      <td>5100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0currentGold  1currentGold  2currentGold  3currentGold  4currentGold  \\\n",
       "0          500.0         500.0         500.0         500.0         500.0   \n",
       "1           25.0         420.0          25.0          25.0          25.0   \n",
       "2          122.0         190.0         115.0         122.0          68.0   \n",
       "3          356.0         634.0         486.0         521.0         276.0   \n",
       "4          128.0        1475.0         128.0         750.0         485.0   \n",
       "5          453.0         212.0         454.0          57.0          80.0   \n",
       "6          803.0         630.0         751.0         439.0         214.0   \n",
       "7         1316.0        1366.0          37.0          23.0         534.0   \n",
       "8           89.0         231.0         361.0         323.0         119.0   \n",
       "9          367.0         395.0        1114.0         968.0         607.0   \n",
       "10         988.0         889.0         148.0        1380.0         836.0   \n",
       "11         554.0         241.0         638.0         177.0          70.0   \n",
       "12        1503.0         655.0         927.0         624.0         246.0   \n",
       "13         535.0        1321.0         331.0         893.0         425.0   \n",
       "14        1279.0         118.0         538.0        1742.0         195.0   \n",
       "15         717.0         640.0         800.0         194.0         474.0   \n",
       "16        1188.0        1057.0        1402.0         699.0         464.0   \n",
       "17         502.0        1327.0          65.0          21.0         634.0   \n",
       "18         625.0        1579.0         432.0         584.0         125.0   \n",
       "19        1020.0        1803.0         140.0        1412.0          65.0   \n",
       "\n",
       "    5currentGold  6currentGold  7currentGold  8currentGold  9currentGold  ...  \\\n",
       "0          500.0         500.0         500.0         500.0         500.0  ...   \n",
       "1            0.0           0.0           0.0           0.0           0.0  ...   \n",
       "2           41.0         144.0         125.0          83.0          22.0  ...   \n",
       "3          359.0         540.0         380.0         373.0         198.0  ...   \n",
       "4          816.0          99.0          89.0         648.0          94.0  ...   \n",
       "5          264.0         413.0         379.0        1050.0         228.0  ...   \n",
       "6          527.0         696.0         750.0         273.0         429.0  ...   \n",
       "7          817.0         198.0          74.0         224.0          29.0  ...   \n",
       "8          308.0         946.0        1129.0         674.0         238.0  ...   \n",
       "9          557.0          75.0          57.0         954.0         195.0  ...   \n",
       "10         905.0         648.0         329.0        1307.0         371.0  ...   \n",
       "11         215.0         286.0         372.0         207.0         632.0  ...   \n",
       "12         379.0         815.0         944.0         413.0         826.0  ...   \n",
       "13         648.0        1585.0         296.0         881.0        1025.0  ...   \n",
       "14         771.0        2098.0        1083.0        1359.0        1436.0  ...   \n",
       "15        1517.0          35.0         652.0         815.0         207.0  ...   \n",
       "16         204.0         510.0         931.0        1243.0         197.0  ...   \n",
       "17         549.0        1509.0         639.0        1722.0         694.0  ...   \n",
       "18        1109.0        2497.0         851.0          45.0         234.0  ...   \n",
       "19          13.0        2805.0        1123.0         457.0         519.0  ...   \n",
       "\n",
       "         1y       3y      4y       5y       6y       7y       8y       9y  \\\n",
       "0     361.0    471.0   649.0  14511.0  14291.0  14223.0  14401.0  14579.0   \n",
       "1     542.0   1013.0   679.0  13885.0  11463.0   8416.0   7493.0  11457.0   \n",
       "2    8042.0   2285.0  2107.0  12344.0  10103.0   7015.0   3101.0   2597.0   \n",
       "3    5011.0   3118.0  3266.0  13855.0   4991.0   7337.0   3991.0   4151.0   \n",
       "4    3263.0   2921.0  2816.0  10840.0   9761.0   9040.0   3937.0  11836.0   \n",
       "5    7169.0   1556.0  1712.0  12890.0   8567.0   8337.0   3373.0   3711.0   \n",
       "6   12602.0   1340.0  4032.0  13411.0   4964.0   7590.0   2309.0   5108.0   \n",
       "7    5325.0    554.0  5437.0  12869.0  11326.0   8390.0   4371.0  12836.0   \n",
       "8   10843.0   2752.0  5456.0   8475.0  10091.0   9822.0   5626.0   9909.0   \n",
       "9    2446.0   3844.0  3924.0  13650.0   8741.0  13681.0   4822.0   4766.0   \n",
       "10  11534.0   4371.0  4248.0   9874.0   9630.0   9898.0   5467.0   6585.0   \n",
       "11   8858.0   1565.0  4859.0  11640.0   6555.0   7502.0   3013.0   3186.0   \n",
       "12   6039.0   3058.0  3066.0  13843.0   4702.0   6947.0   3803.0   5725.0   \n",
       "13  13541.0   3039.0  3873.0  14048.0  13329.0   8733.0   5793.0   4492.0   \n",
       "14   7701.0   5947.0  7509.0  12368.0   8283.0   8230.0   7160.0   9050.0   \n",
       "15   3963.0   7280.0   482.0   8152.0  10842.0  13602.0   8196.0   8667.0   \n",
       "16   6598.0   7373.0  7847.0  11955.0  10590.0   8602.0   9626.0   9397.0   \n",
       "17   6385.0   6863.0  1107.0  14025.0  10583.0  11714.0   7330.0   7373.0   \n",
       "18   5060.0  12398.0  6453.0   6463.0   6437.0   6681.0   7231.0   6545.0   \n",
       "19   8438.0  13454.0  2322.0  14074.0   1422.0   1340.0   8120.0   1559.0   \n",
       "\n",
       "    player_x  player_y  \n",
       "0      351.0     293.0  \n",
       "1     6348.0    9817.0  \n",
       "2     6598.0    6970.0  \n",
       "3     7235.0    6689.0  \n",
       "4     7473.0    7689.0  \n",
       "5     8044.0    7598.0  \n",
       "6     6946.0    7036.0  \n",
       "7     2871.0    2268.0  \n",
       "8     7742.0    7255.0  \n",
       "9     8023.0    7885.0  \n",
       "10    1252.0    1886.0  \n",
       "11    7075.0    6922.0  \n",
       "12    6208.0    6108.0  \n",
       "13    7816.0    7999.0  \n",
       "14    1256.0    1095.0  \n",
       "15    7133.0    7148.0  \n",
       "16    9254.0    7471.0  \n",
       "17    2815.0    2201.0  \n",
       "18    7628.0    6715.0  \n",
       "19    5584.0    5100.0  \n",
       "\n",
       "[20 rows x 60 columns]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "    def __init__(self, input_width, label_width, shift,\n",
    "                 full_df=test0, val_df = test1,\n",
    "                 dfdi=dfs_dict,\n",
    "                 #train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "                 label_columns=None):\n",
    "        # Store the raw data.\n",
    "        #self.train_df = train_df\n",
    "        #self.val_df = val_df\n",
    "        #self.test_df = test_df\n",
    "        self.full_df = full_df\n",
    "        self.val_df = val_df\n",
    "        self.dfdi = dfdi\n",
    "        self.trainli = None\n",
    "        self.valli = None\n",
    "        self.testli = None\n",
    "\n",
    "        # Work out the label column indices.\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {name: i for i, name in\n",
    "                                                                        enumerate(label_columns)}\n",
    "        self.column_indices = {name: i for i, name in\n",
    "                                                     enumerate(full_df.columns)}\n",
    "\n",
    "        # Work out the window parameters.\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "\n",
    "        self.total_window_size = input_width + shift\n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "                f'Total window size: {self.total_window_size}',\n",
    "                f'Input indices: {self.input_indices}',\n",
    "                f'Label indices: {self.label_indices}',\n",
    "                f'Label column name(s): {self.label_columns}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_window(self, features):\n",
    "    inputs = features[:, self.input_slice, :]\n",
    "    labels = features[:, self.labels_slice, :]\n",
    "    if self.label_columns is not None:\n",
    "        labels = tf.stack(\n",
    "                [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "                axis=-1)\n",
    "    \n",
    "\n",
    "    # Slicing doesn't preserve static shape information, so set the shapes\n",
    "    # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "    inputs.set_shape([None, self.input_width, None])\n",
    "    labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "    return inputs, labels\n",
    "\n",
    "WindowGenerator.split_window = split_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(self, model=None, plot_col=\"player_x\", max_subplots=3):\n",
    "    inputs, labels = self.example\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plot_col_index = self.column_indices[plot_col]\n",
    "    print(plot_col_index, type(plot_col_index))\n",
    "    max_n = min(max_subplots, len(inputs))\n",
    "    for n in range(max_n):\n",
    "        plt.subplot(3, 1, n+1)\n",
    "        plt.ylabel(f'{plot_col} [normed]')\n",
    "        plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n",
    "                         label='Inputs', marker='.', zorder=-10)\n",
    "\n",
    "        if self.label_columns:\n",
    "            print(type(self.label_columns_indices))\n",
    "            label_col_index = self.label_columns_indices.get(plot_col, None)\n",
    "            print(label_col_index)\n",
    "        else:\n",
    "            label_col_index = plot_col_index\n",
    "\n",
    "        if label_col_index is None:\n",
    "            continue\n",
    "\n",
    "        print(self.label_indices)\n",
    "        plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
    "                                edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "        if model is not None:\n",
    "            predictions = model(inputs)\n",
    "            plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n",
    "                                    marker='X', edgecolors='k', label='Predictions',\n",
    "                                    c='#ff7f0e', s=64)\n",
    "\n",
    "        if n == 0:\n",
    "            plt.legend()\n",
    "\n",
    "    plt.xlabel('Time [h]')\n",
    "\n",
    "WindowGenerator.plot = plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(self, data, list_me=False):\n",
    "    data = np.array(data, dtype=np.float32)\n",
    "    ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "            data=data,\n",
    "            targets=None,\n",
    "            sequence_length=self.total_window_size,\n",
    "            sequence_stride=1,\n",
    "            shuffle=True,\n",
    "            batch_size=32,)\n",
    "\n",
    "    ds = ds.map(self.split_window)\n",
    "    if list_me:\n",
    "        return list(ds)\n",
    "    return ds\n",
    "\n",
    "WindowGenerator.make_dataset = make_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_split_from_dfdi(self,splits=(.8,.9),minmax=(0,20)):\n",
    "    # minmax is start and end time of window being sampled from.\n",
    "    # splits is where the data is being split for train val test split\n",
    "    if self.dfdi is None:\n",
    "        print(\"no dfdi attached\")\n",
    "        raise KeyError(\"no dfdi attached\")\n",
    "    gameids = list(self.dfdi.keys())\n",
    "    gameids.sort()\n",
    "    train_ids = gameids[0:int(len(gameids)*splits[0])]\n",
    "    val_ids = gameids[int(len(gameids)*splits[0]):int(len(gameids)*splits[1])]\n",
    "    test_ids = gameids[int(len(gameids)*splits[1]):]\n",
    "    if self.trainli is None:\n",
    "        trainli = build_batch(self, self.dfdi, train_ids, minmax)\n",
    "        train_feats = tf.concat([batch[0] for batch in trainli], axis=0)\n",
    "        train_labels = tf.concat([batch[1] for batch in trainli], axis=0)\n",
    "        \n",
    "    if self.valli is None:\n",
    "        valli = build_batch(self, self.dfdi, val_ids, minmax)\n",
    "        val_feats = tf.concat([batch[0] for batch in valli], axis=0)\n",
    "        val_labels = tf.concat([batch[1] for batch in valli], axis=0)\n",
    "            \n",
    "    if self.testli is None:\n",
    "        testli = build_batch(self, self.dfdi, test_ids, minmax)\n",
    "        test_feats = tf.concat([batch[0] for batch in testli], axis=0)\n",
    "        test_labels = tf.concat([batch[1] for batch in testli], axis=0)\n",
    "            \n",
    "        \n",
    "    print(train_feats.shape, train_labels.shape)\n",
    "    self.train = (train_feats, train_labels)\n",
    "    self.val = (val_feats, val_labels)\n",
    "    self.test = (test_feats, test_labels)\n",
    "\n",
    "WindowGenerator.make_split_from_dfdi = make_split_from_dfdi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "@property\n",
    "def example(self):\n",
    "    \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "    result = getattr(self, '_example', None)\n",
    "    if result is None:\n",
    "        # No example batch was found, so get one from the `.train` dataset\n",
    "        result = next(iter(self.train))\n",
    "        # And cache it for next time\n",
    "        self._example = result\n",
    "    return result\n",
    "\n",
    "WindowGenerator.example = example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_batch(self, dfdi, id_li, minmax):\n",
    "    dataset_li = []\n",
    "    for match_id in id_li:\n",
    "        df = dfdi[match_id]\n",
    "        df = df.drop([\"timestamp\"], axis=1)\n",
    "        df = df.iloc[minmax[0]:minmax[1]]\n",
    "        dataset_li = dataset_li + make_dataset(self, data=df, list_me=True)\n",
    "    return dataset_li\n",
    "\n",
    "WindowGenerator.build_batch = build_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['0currentGold', '1currentGold', '2currentGold', '3currentGold',\n",
       "       '4currentGold', '5currentGold', '6currentGold', '7currentGold',\n",
       "       '8currentGold', '9currentGold', '0totalGold', '1totalGold',\n",
       "       '2totalGold', '3totalGold', '4totalGold', '5totalGold', '6totalGold',\n",
       "       '7totalGold', '8totalGold', '9totalGold', '0level', '1level', '2level',\n",
       "       '3level', '4level', '5level', '6level', '7level', '8level', '9level',\n",
       "       '0jungleMinionsKilled', '1jungleMinionsKilled', '2jungleMinionsKilled',\n",
       "       '3jungleMinionsKilled', '4jungleMinionsKilled', '5jungleMinionsKilled',\n",
       "       '6jungleMinionsKilled', '7jungleMinionsKilled', '8jungleMinionsKilled',\n",
       "       '9jungleMinionsKilled', '0x', '1x', '3x', '4x', '5x', '6x', '7x', '8x',\n",
       "       '9x', '0y', '1y', '3y', '4y', '5y', '6y', '7y', '8y', '9y', 'player_x',\n",
       "       'player_y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test0.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Total window size: 7\n",
       "Input indices: [0 1 2 3 4]\n",
       "Label indices: [5 6]\n",
       "Label column name(s): ['player_x', 'player_y']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2 = WindowGenerator(input_width=5, label_width=2, shift=2,\n",
    "                     label_columns=[\"player_x\",\"player_y\"])\n",
    "w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Total window size: 2\n",
       "Input indices: [0]\n",
       "Label indices: [1]\n",
       "Label column name(s): ['player_x', 'player_y']"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_step_window = WindowGenerator(\n",
    "    input_width=1, label_width=1, shift=1,\n",
    "    label_columns=[\"player_x\", \"player_y\"])#,\n",
    "single_step_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1824, 1, 60) (1824, 1, 2)\n"
     ]
    }
   ],
   "source": [
    "single_step_window.make_split_from_dfdi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_distance_mae(y_true, y_pred):\n",
    "    \"\"\"takes tuple of 2 tensors and returns the sum distance between the points\"\"\"\n",
    "    dx2 = (y_true[:,:,0] - y_pred[:,:,0])**2\n",
    "    dy2 = (y_true[:,:,1] - y_pred[:,:,1])**2\n",
    "    dist = (dx2 + dy2)**.5\n",
    "    dist_sum = tf.math.reduce_mean(dist)\n",
    "    return dist_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_distance_rmse(y_true, y_pred):\n",
    "    \"\"\"takes tuple of 2 tensors and returns the sum distance between the points\"\"\"\n",
    "    dx2 = (y_true[:,:,0] - y_pred[:,:,0])**2\n",
    "    dy2 = (y_true[:,:,1] - y_pred[:,:,1])**2\n",
    "    dist2 = dx2 + dy2\n",
    "    dist2_sum = tf.math.reduce_mean(dist2)\n",
    "    rmse = dist2_sum**.5\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def tensor_distance_rmse_pure(y_true, y_pred):\\n    dx2 = (y_true[:,:,0] - y_pred[:,:,0])**2\\n    dy2 = (y_true[:,:,1] - y_pred[:,:,1])**2\\n    dist2 = dx2 + dy2\\n    dist2_sum = tf.math.reduce_sum(dist2)\\n    rmse = dist2_sum**.5\\n    return tf.pow(tf.math.reduce_sum(tf.pow(y_true[:,:,0] - y_pred[:,:,0], 2)\\n                                     + dy2),.5)'"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def tensor_distance_rmse_pure(y_true, y_pred):\n",
    "    dx2 = (y_true[:,:,0] - y_pred[:,:,0])**2\n",
    "    dy2 = (y_true[:,:,1] - y_pred[:,:,1])**2\n",
    "    dist2 = dx2 + dy2\n",
    "    dist2_sum = tf.math.reduce_sum(dist2)\n",
    "    rmse = dist2_sum**.5\n",
    "    return tf.pow(tf.math.reduce_sum(tf.pow(y_true[:,:,0] - y_pred[:,:,0], 2)\n",
    "                                     + dy2),.5)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline(tf.keras.Model):\n",
    "    def __init__(self, label_index=None):\n",
    "        super().__init__()\n",
    "        self.label_index = label_index\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if self.label_index is None:\n",
    "            return inputs\n",
    "        elif isinstance(self.label_index,(list,tuple)):\n",
    "            return inputs[:,:, self.label_index[0]:self.label_index[1]+1]\n",
    "        result = inputs[:, :, self.label_index]\n",
    "        return result[:, :, tf.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hold = single_step_window.train# = tf.data.Dataset.from_tensor_slices([train_feats, train_labels])\n",
    "val_hold = single_step_window.train\n",
    "test_hold = single_step_window.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1824, 1, 2), dtype=float32, numpy=\n",
       "array([[[ 9254.,  7471.]],\n",
       "\n",
       "       [[ 6598.,  6970.]],\n",
       "\n",
       "       [[ 8023.,  7885.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 7577.,  7104.]],\n",
       "\n",
       "       [[ 8755.,  3754.]],\n",
       "\n",
       "       [[12450.,  2352.]]], dtype=float32)>"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_hold[1]#[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = Baseline(label_index=[column_indices[\"player_x\"],column_indices[\"player_y\"]])#[column_indices[\"player_x\"],column_indices[\"player_y\"]])\n",
    "baseline.compile(loss=tensor_distance_mae,\n",
    "                 metrics=tensor_distance_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear.compile(loss=tensor_distance_mae,\n",
    "               metrics=tensor_distance_mae,\n",
    "               optimizer=tf.optimizers.Adam()\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense.compile(loss=tensor_distance_mae,\n",
    "               metrics=tensor_distance_mae,\n",
    "               optimizer=tf.optimizers.Adam()\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "57/57 [==============================] - 0s 5ms/step - loss: 4719.3911 - tensor_distance_mae: 4719.3911 - val_loss: 4719.3911 - val_tensor_distance_mae: 4719.3911\n",
      "Epoch 2/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 4719.3901 - tensor_distance_mae: 4719.3901 - val_loss: 4719.3911 - val_tensor_distance_mae: 4719.3911\n",
      "Epoch 3/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 4719.3896 - tensor_distance_mae: 4719.3896 - val_loss: 4719.3911 - val_tensor_distance_mae: 4719.3911\n",
      "Epoch 4/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 4719.3911 - tensor_distance_mae: 4719.3911 - val_loss: 4719.3911 - val_tensor_distance_mae: 4719.3911\n",
      "Epoch 5/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 4719.3916 - tensor_distance_mae: 4719.3916 - val_loss: 4719.3911 - val_tensor_distance_mae: 4719.3911\n",
      "Epoch 6/30\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 4719.3911 - tensor_distance_mae: 4719.3911 - val_loss: 4719.3911 - val_tensor_distance_mae: 4719.3911\n",
      "Epoch 7/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 4719.3911 - tensor_distance_mae: 4719.3911 - val_loss: 4719.3911 - val_tensor_distance_mae: 4719.3911\n",
      "Epoch 8/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 4719.3916 - tensor_distance_mae: 4719.3916 - val_loss: 4719.3911 - val_tensor_distance_mae: 4719.3911\n",
      "Epoch 9/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 4719.3916 - tensor_distance_mae: 4719.3916 - val_loss: 4719.3911 - val_tensor_distance_mae: 4719.3911\n",
      "Epoch 10/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 4719.3911 - tensor_distance_mae: 4719.3911 - val_loss: 4719.3911 - val_tensor_distance_mae: 4719.3911\n",
      "Epoch 11/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 4719.3911 - tensor_distance_mae: 4719.3911 - val_loss: 4719.3911 - val_tensor_distance_mae: 4719.3911\n",
      "Epoch 12/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 4719.3901 - tensor_distance_mae: 4719.3901 - val_loss: 4719.3911 - val_tensor_distance_mae: 4719.3911\n",
      "Epoch 13/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 4719.3921 - tensor_distance_mae: 4719.3921 - val_loss: 4719.3911 - val_tensor_distance_mae: 4719.3911\n",
      "Epoch 14/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 4719.3911 - tensor_distance_mae: 4719.3911 - val_loss: 4719.3911 - val_tensor_distance_mae: 4719.3911\n",
      "Epoch 15/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 4719.3911 - tensor_distance_mae: 4719.3911 - val_loss: 4719.3911 - val_tensor_distance_mae: 4719.3911\n",
      "Epoch 16/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 4719.3911 - tensor_distance_mae: 4719.3911 - val_loss: 4719.3911 - val_tensor_distance_mae: 4719.3911\n",
      "Epoch 17/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 4719.3911 - tensor_distance_mae: 4719.3911 - val_loss: 4719.3911 - val_tensor_distance_mae: 4719.3911\n",
      "Epoch 18/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 4719.3921 - tensor_distance_mae: 4719.3921 - val_loss: 4719.3911 - val_tensor_distance_mae: 4719.3911\n",
      "Epoch 19/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 4719.3911 - tensor_distance_mae: 4719.3911 - val_loss: 4719.3911 - val_tensor_distance_mae: 4719.3911\n",
      "Epoch 20/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 4719.3901 - tensor_distance_mae: 4719.3901 - val_loss: 4719.3911 - val_tensor_distance_mae: 4719.3911\n",
      "Epoch 21/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 4719.3916 - tensor_distance_mae: 4719.3916 - val_loss: 4719.3911 - val_tensor_distance_mae: 4719.3911\n",
      "Epoch 22/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 4719.3901 - tensor_distance_mae: 4719.3901 - val_loss: 4719.3911 - val_tensor_distance_mae: 4719.3911\n",
      "Epoch 23/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 4719.3911 - tensor_distance_mae: 4719.3911 - val_loss: 4719.3911 - val_tensor_distance_mae: 4719.3911\n",
      "Epoch 24/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 4719.3901 - tensor_distance_mae: 4719.3901 - val_loss: 4719.3911 - val_tensor_distance_mae: 4719.3911\n",
      "Epoch 25/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 4719.3911 - tensor_distance_mae: 4719.3911 - val_loss: 4719.3911 - val_tensor_distance_mae: 4719.3911\n",
      "Epoch 26/30\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 4719.3901 - tensor_distance_mae: 4719.3901 - val_loss: 4719.3911 - val_tensor_distance_mae: 4719.3911\n",
      "Epoch 27/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 4719.3901 - tensor_distance_mae: 4719.3901 - val_loss: 4719.3911 - val_tensor_distance_mae: 4719.3911\n",
      "Epoch 28/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 4719.3911 - tensor_distance_mae: 4719.3911 - val_loss: 4719.3911 - val_tensor_distance_mae: 4719.3911\n",
      "Epoch 29/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 4719.3911 - tensor_distance_mae: 4719.3911 - val_loss: 4719.3911 - val_tensor_distance_mae: 4719.3911\n",
      "Epoch 30/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 4719.3911 - tensor_distance_mae: 4719.3911 - val_loss: 4719.3911 - val_tensor_distance_mae: 4719.3911\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb8cdc36150>"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline.fit(x=train_hold[0],\n",
    "           y=train_hold[1],\n",
    "           batch_size=32,\n",
    "           epochs=30,\n",
    "           verbose=1,\n",
    "           validation_data=val_hold\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3835.2539 - tensor_distance_mae: 3835.2539 - val_loss: 3801.8513 - val_tensor_distance_mae: 3801.8513\n",
      "Epoch 2/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3805.3037 - tensor_distance_mae: 3805.3037 - val_loss: 3791.2224 - val_tensor_distance_mae: 3791.2224\n",
      "Epoch 3/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3799.9502 - tensor_distance_mae: 3799.9502 - val_loss: 3784.4036 - val_tensor_distance_mae: 3784.4036\n",
      "Epoch 4/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3792.7280 - tensor_distance_mae: 3792.7280 - val_loss: 3772.1521 - val_tensor_distance_mae: 3772.1521\n",
      "Epoch 5/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3776.5063 - tensor_distance_mae: 3776.5063 - val_loss: 3760.7429 - val_tensor_distance_mae: 3760.7429\n",
      "Epoch 6/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3768.9814 - tensor_distance_mae: 3768.9814 - val_loss: 3762.4851 - val_tensor_distance_mae: 3762.4851\n",
      "Epoch 7/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3762.5989 - tensor_distance_mae: 3762.5989 - val_loss: 3749.3721 - val_tensor_distance_mae: 3749.3721\n",
      "Epoch 8/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 3760.3101 - tensor_distance_mae: 3760.3101 - val_loss: 3745.8188 - val_tensor_distance_mae: 3745.8188\n",
      "Epoch 9/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 3757.2429 - tensor_distance_mae: 3757.2429 - val_loss: 3760.7839 - val_tensor_distance_mae: 3760.7839\n",
      "Epoch 10/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3755.4626 - tensor_distance_mae: 3755.4626 - val_loss: 3740.1895 - val_tensor_distance_mae: 3740.1895\n",
      "Epoch 11/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3748.1460 - tensor_distance_mae: 3748.1460 - val_loss: 3756.3389 - val_tensor_distance_mae: 3756.3389\n",
      "Epoch 12/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3752.1182 - tensor_distance_mae: 3752.1182 - val_loss: 3741.0549 - val_tensor_distance_mae: 3741.0549\n",
      "Epoch 13/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 3748.2388 - tensor_distance_mae: 3748.2388 - val_loss: 3739.5630 - val_tensor_distance_mae: 3739.5630\n",
      "Epoch 14/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3749.8562 - tensor_distance_mae: 3749.8562 - val_loss: 3741.5398 - val_tensor_distance_mae: 3741.5398\n",
      "Epoch 15/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3744.2324 - tensor_distance_mae: 3744.2324 - val_loss: 3752.2954 - val_tensor_distance_mae: 3752.2954\n",
      "Epoch 16/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 3746.3345 - tensor_distance_mae: 3746.3345 - val_loss: 3738.2454 - val_tensor_distance_mae: 3738.2454\n",
      "Epoch 17/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 3744.3264 - tensor_distance_mae: 3744.3264 - val_loss: 3746.5959 - val_tensor_distance_mae: 3746.5959\n",
      "Epoch 18/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 3740.6453 - tensor_distance_mae: 3740.6453 - val_loss: 3742.5469 - val_tensor_distance_mae: 3742.5469\n",
      "Epoch 19/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3749.2434 - tensor_distance_mae: 3749.2434 - val_loss: 3738.3333 - val_tensor_distance_mae: 3738.3333\n",
      "Epoch 20/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3737.1033 - tensor_distance_mae: 3737.1033 - val_loss: 3733.0488 - val_tensor_distance_mae: 3733.0488\n",
      "Epoch 21/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3741.3721 - tensor_distance_mae: 3741.3721 - val_loss: 3730.2563 - val_tensor_distance_mae: 3730.2563\n",
      "Epoch 22/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3735.6899 - tensor_distance_mae: 3735.6899 - val_loss: 3727.4673 - val_tensor_distance_mae: 3727.4673\n",
      "Epoch 23/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3737.5129 - tensor_distance_mae: 3737.5129 - val_loss: 3728.9158 - val_tensor_distance_mae: 3728.9158\n",
      "Epoch 24/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 3730.5527 - tensor_distance_mae: 3730.5527 - val_loss: 3729.5449 - val_tensor_distance_mae: 3729.5449\n",
      "Epoch 25/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3748.5906 - tensor_distance_mae: 3748.5906 - val_loss: 3725.4460 - val_tensor_distance_mae: 3725.4460\n",
      "Epoch 26/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3734.0833 - tensor_distance_mae: 3734.0833 - val_loss: 3724.0208 - val_tensor_distance_mae: 3724.0208\n",
      "Epoch 27/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3737.5850 - tensor_distance_mae: 3737.5850 - val_loss: 3721.3567 - val_tensor_distance_mae: 3721.3567\n",
      "Epoch 28/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3733.4495 - tensor_distance_mae: 3733.4495 - val_loss: 3721.1533 - val_tensor_distance_mae: 3721.1533\n",
      "Epoch 29/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 3730.4221 - tensor_distance_mae: 3730.4221 - val_loss: 3722.2783 - val_tensor_distance_mae: 3722.2783\n",
      "Epoch 30/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 3738.2554 - tensor_distance_mae: 3738.2554 - val_loss: 3734.4343 - val_tensor_distance_mae: 3734.4343\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb8d206bc50>"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear.fit(x=train_hold[0],\n",
    "           y=train_hold[1],\n",
    "           batch_size=32,\n",
    "           epochs=30,\n",
    "           verbose=1,\n",
    "           validation_data=val_hold\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3395.8005 - tensor_distance_mae: 3395.8005 - val_loss: 3272.8000 - val_tensor_distance_mae: 3272.8000\n",
      "Epoch 2/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3314.6870 - tensor_distance_mae: 3314.6870 - val_loss: 3271.4524 - val_tensor_distance_mae: 3271.4524\n",
      "Epoch 3/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3317.9678 - tensor_distance_mae: 3317.9678 - val_loss: 3275.7751 - val_tensor_distance_mae: 3275.7751\n",
      "Epoch 4/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3327.2930 - tensor_distance_mae: 3327.2930 - val_loss: 3251.8977 - val_tensor_distance_mae: 3251.8977\n",
      "Epoch 5/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3290.1982 - tensor_distance_mae: 3290.1982 - val_loss: 3206.0386 - val_tensor_distance_mae: 3206.0386\n",
      "Epoch 6/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3299.4260 - tensor_distance_mae: 3299.4260 - val_loss: 3200.5706 - val_tensor_distance_mae: 3200.5706\n",
      "Epoch 7/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 3257.2283 - tensor_distance_mae: 3257.2283 - val_loss: 3265.4226 - val_tensor_distance_mae: 3265.4226\n",
      "Epoch 8/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 3271.1941 - tensor_distance_mae: 3271.1941 - val_loss: 3212.3213 - val_tensor_distance_mae: 3212.3213\n",
      "Epoch 9/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 3224.3689 - tensor_distance_mae: 3224.3689 - val_loss: 3169.0759 - val_tensor_distance_mae: 3169.0759\n",
      "Epoch 10/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3231.5488 - tensor_distance_mae: 3231.5488 - val_loss: 3169.7161 - val_tensor_distance_mae: 3169.7161\n",
      "Epoch 11/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 3202.9277 - tensor_distance_mae: 3202.9277 - val_loss: 3315.9348 - val_tensor_distance_mae: 3315.9348\n",
      "Epoch 12/30\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3188.3340 - tensor_distance_mae: 3188.3340 - val_loss: 3134.9590 - val_tensor_distance_mae: 3134.9590\n",
      "Epoch 13/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 3192.0833 - tensor_distance_mae: 3192.0833 - val_loss: 3113.0669 - val_tensor_distance_mae: 3113.0669\n",
      "Epoch 14/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3164.1584 - tensor_distance_mae: 3164.1584 - val_loss: 3086.0520 - val_tensor_distance_mae: 3086.0520\n",
      "Epoch 15/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3124.7908 - tensor_distance_mae: 3124.7908 - val_loss: 3059.2446 - val_tensor_distance_mae: 3059.2446\n",
      "Epoch 16/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3119.3079 - tensor_distance_mae: 3119.3079 - val_loss: 3044.6946 - val_tensor_distance_mae: 3044.6946\n",
      "Epoch 17/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3117.6970 - tensor_distance_mae: 3117.6970 - val_loss: 3092.0571 - val_tensor_distance_mae: 3092.0571\n",
      "Epoch 18/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 3116.5754 - tensor_distance_mae: 3116.5754 - val_loss: 3040.5322 - val_tensor_distance_mae: 3040.5322\n",
      "Epoch 19/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 3109.2937 - tensor_distance_mae: 3109.2937 - val_loss: 3028.3052 - val_tensor_distance_mae: 3028.3052\n",
      "Epoch 20/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3073.5581 - tensor_distance_mae: 3073.5581 - val_loss: 3002.5808 - val_tensor_distance_mae: 3002.5808\n",
      "Epoch 21/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 3041.1833 - tensor_distance_mae: 3041.1833 - val_loss: 3032.6807 - val_tensor_distance_mae: 3032.6807\n",
      "Epoch 22/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3038.9392 - tensor_distance_mae: 3038.9392 - val_loss: 2998.3438 - val_tensor_distance_mae: 2998.3438\n",
      "Epoch 23/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3018.3101 - tensor_distance_mae: 3018.3101 - val_loss: 2972.0781 - val_tensor_distance_mae: 2972.0781\n",
      "Epoch 24/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 3055.9854 - tensor_distance_mae: 3055.9854 - val_loss: 2944.8821 - val_tensor_distance_mae: 2944.8821\n",
      "Epoch 25/30\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3002.2451 - tensor_distance_mae: 3002.2451 - val_loss: 2932.2112 - val_tensor_distance_mae: 2932.2112\n",
      "Epoch 26/30\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 2975.2878 - tensor_distance_mae: 2975.2878 - val_loss: 2929.9329 - val_tensor_distance_mae: 2929.9329\n",
      "Epoch 27/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 2963.9771 - tensor_distance_mae: 2963.9771 - val_loss: 2912.9963 - val_tensor_distance_mae: 2912.9963\n",
      "Epoch 28/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 2971.9233 - tensor_distance_mae: 2971.9233 - val_loss: 2905.2759 - val_tensor_distance_mae: 2905.2759\n",
      "Epoch 29/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 2971.7302 - tensor_distance_mae: 2971.7302 - val_loss: 2939.8586 - val_tensor_distance_mae: 2939.8586\n",
      "Epoch 30/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 2972.1018 - tensor_distance_mae: 2972.1018 - val_loss: 2922.9785 - val_tensor_distance_mae: 2922.9785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb8d2451990>"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense.fit(x=train_hold[0],\n",
    "           y=train_hold[1],\n",
    "           batch_size=32,\n",
    "           epochs=30,\n",
    "           verbose=1,\n",
    "           validation_data=val_hold\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Total window size: 4\n",
       "Input indices: [0 1 2]\n",
       "Label indices: [3]\n",
       "Label column name(s): ['player_x', 'player_y']"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONV_WIDTH = 3\n",
    "conv_window = WindowGenerator(\n",
    "    input_width=CONV_WIDTH,\n",
    "    label_width=1,\n",
    "    shift=1,\n",
    "    label_columns=[\"player_x\",\"player_y\"])\n",
    "\n",
    "conv_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "57/57 [==============================] - 0s 7ms/step - loss: 5780.5728 - tensor_distance_mae: 5780.5728 - val_loss: 4549.3442 - val_tensor_distance_mae: 4549.3442\n",
      "Epoch 2/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 4435.4526 - tensor_distance_mae: 4435.4526 - val_loss: 4252.7021 - val_tensor_distance_mae: 4252.7021\n",
      "Epoch 3/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 4139.9985 - tensor_distance_mae: 4139.9985 - val_loss: 3970.9265 - val_tensor_distance_mae: 3970.9265\n",
      "Epoch 4/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 3936.2051 - tensor_distance_mae: 3936.2051 - val_loss: 3835.2871 - val_tensor_distance_mae: 3835.2871\n",
      "Epoch 5/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 3837.0645 - tensor_distance_mae: 3837.0645 - val_loss: 3770.2722 - val_tensor_distance_mae: 3770.2722\n",
      "Epoch 6/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 3815.5283 - tensor_distance_mae: 3815.5283 - val_loss: 3774.9036 - val_tensor_distance_mae: 3774.9036\n",
      "Epoch 7/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 3766.9607 - tensor_distance_mae: 3766.9607 - val_loss: 3720.4868 - val_tensor_distance_mae: 3720.4868\n",
      "Epoch 8/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 3760.8462 - tensor_distance_mae: 3760.8462 - val_loss: 3786.5671 - val_tensor_distance_mae: 3786.5671\n",
      "Epoch 9/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3751.6052 - tensor_distance_mae: 3751.6052 - val_loss: 3691.3088 - val_tensor_distance_mae: 3691.3088\n",
      "Epoch 10/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3691.1414 - tensor_distance_mae: 3691.1414 - val_loss: 3686.2246 - val_tensor_distance_mae: 3686.2246\n",
      "Epoch 11/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3699.1687 - tensor_distance_mae: 3699.1687 - val_loss: 3724.9238 - val_tensor_distance_mae: 3724.9238\n",
      "Epoch 12/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3691.8625 - tensor_distance_mae: 3691.8625 - val_loss: 3738.1221 - val_tensor_distance_mae: 3738.1221\n",
      "Epoch 13/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3684.2151 - tensor_distance_mae: 3684.2151 - val_loss: 3625.7742 - val_tensor_distance_mae: 3625.7742\n",
      "Epoch 14/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3671.4216 - tensor_distance_mae: 3671.4216 - val_loss: 3620.5916 - val_tensor_distance_mae: 3620.5916\n",
      "Epoch 15/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3661.4338 - tensor_distance_mae: 3661.4338 - val_loss: 3639.0132 - val_tensor_distance_mae: 3639.0132\n",
      "Epoch 16/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3641.3689 - tensor_distance_mae: 3641.3689 - val_loss: 3602.2185 - val_tensor_distance_mae: 3602.2185\n",
      "Epoch 17/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3636.5303 - tensor_distance_mae: 3636.5303 - val_loss: 3582.0779 - val_tensor_distance_mae: 3582.0779\n",
      "Epoch 18/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3614.9324 - tensor_distance_mae: 3614.9324 - val_loss: 3602.6255 - val_tensor_distance_mae: 3602.6255\n",
      "Epoch 19/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3614.2051 - tensor_distance_mae: 3614.2051 - val_loss: 3587.5715 - val_tensor_distance_mae: 3587.5715\n",
      "Epoch 20/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3605.2563 - tensor_distance_mae: 3605.2563 - val_loss: 3569.1711 - val_tensor_distance_mae: 3569.1711\n",
      "Epoch 21/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3582.8228 - tensor_distance_mae: 3582.8228 - val_loss: 3574.9556 - val_tensor_distance_mae: 3574.9556\n",
      "Epoch 22/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3590.9595 - tensor_distance_mae: 3590.9595 - val_loss: 3559.6523 - val_tensor_distance_mae: 3559.6523\n",
      "Epoch 23/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3582.8420 - tensor_distance_mae: 3582.8420 - val_loss: 3570.1724 - val_tensor_distance_mae: 3570.1724\n",
      "Epoch 24/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 3581.4697 - tensor_distance_mae: 3581.4697 - val_loss: 3539.2317 - val_tensor_distance_mae: 3539.2317\n",
      "Epoch 25/30\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 3565.7048 - tensor_distance_mae: 3565.7048 - val_loss: 3575.0674 - val_tensor_distance_mae: 3575.0674\n",
      "Epoch 26/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 3576.7598 - tensor_distance_mae: 3576.7598 - val_loss: 3523.2646 - val_tensor_distance_mae: 3523.2646\n",
      "Epoch 27/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 3578.3442 - tensor_distance_mae: 3578.3442 - val_loss: 3611.9907 - val_tensor_distance_mae: 3611.9907\n",
      "Epoch 28/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 3547.4563 - tensor_distance_mae: 3547.4563 - val_loss: 3502.2720 - val_tensor_distance_mae: 3502.2720\n",
      "Epoch 29/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 3539.7407 - tensor_distance_mae: 3539.7407 - val_loss: 3516.2976 - val_tensor_distance_mae: 3516.2976\n",
      "Epoch 30/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 3546.0981 - tensor_distance_mae: 3546.0981 - val_loss: 3521.9834 - val_tensor_distance_mae: 3521.9834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb8d2b1ba90>"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_step_dense = tf.keras.Sequential([\n",
    "    # Shape: (time, features) => (time*features)\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=32, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=32, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=2),\n",
    "    # Add back the time dimension.\n",
    "    # Shape: (outputs) => (1, outputs)\n",
    "    tf.keras.layers.Reshape([1, -1]),\n",
    "])\n",
    "\n",
    "multi_step_dense.compile(loss=tensor_distance_mae,\n",
    "               metrics=tensor_distance_mae,\n",
    "               optimizer=tf.optimizers.Adam()\n",
    "              )\n",
    "\n",
    "multi_step_dense.fit(x=train_hold[0],\n",
    "           y=train_hold[1],\n",
    "           batch_size=32,\n",
    "           epochs=30,\n",
    "           verbose=1,\n",
    "           validation_data=val_hold\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "linear_test_pred = linear.predict(x=test_hold[0])\n",
    "baseline_test_pred = baseline.predict(x=test_hold[0])\n",
    "dense_test_pred = dense.predict(x=test_hold[0])\n",
    "mdense_test_pred = multi_step_dense.predict(x=test_hold[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/8 [==>...........................] - ETA: 0s - loss: 3727.6296 - tensor_distance_mae: 3727.6296WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0013s vs `on_test_batch_end` time: 0.0058s). Check your callbacks.\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 3597.5227 - tensor_distance_mae: 3824.1611\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4565.3867 - tensor_distance_mae: 4832.8428\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 3604.1555 - tensor_distance_mae: 3810.7690\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 3470.7424 - tensor_distance_mae: 3686.0684\n",
      "baseline rmse error: 4832.8427734375\n",
      "linear rmse error: 3824.1611328125\n",
      "dense rmse error: 3810.76904296875\n",
      "multistep dense rmse error: 3686.068359375\n"
     ]
    }
   ],
   "source": [
    "linear_error = linear.evaluate(test_hold[0], test_hold[1])#, batch_size=128)\n",
    "baseline_error = baseline.evaluate(test_hold[0], test_hold[1])\n",
    "dense_error = dense.evaluate(test_hold[0], test_hold[1])\n",
    "mdense_error = multi_step_dense.evaluate(test_hold[0], test_hold[1])\n",
    "print(\"baseline rmse error:\", baseline_error[1])\n",
    "print(\"linear rmse error:\", linear_error[1])\n",
    "print(\"dense rmse error:\", dense_error[1])\n",
    "print(\"multistep dense rmse error:\", mdense_error[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(228, 1, 60), dtype=float32, numpy=\n",
       "array([[[  690.,  1103.,   606., ...,  4542., 13523.,  2841.]],\n",
       "\n",
       "       [[   60.,   762.,  1330., ...,  3861., 13448.,  3137.]],\n",
       "\n",
       "       [[  221.,   909.,    37., ...,  5576., 13473.,  6629.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[  642.,   620.,  1130., ...,  3475.,  3153., 13180.]],\n",
       "\n",
       "       [[   89.,   553.,   238., ...,  8958.,  7904.,  8704.]],\n",
       "\n",
       "       [[ 2451.,   354.,   508., ...,  2165.,  2620., 12212.]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_hold[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[12629.87   ,  2373.7356 ]],\n",
       "\n",
       "       [[11622.855  ,  2728.806  ]],\n",
       "\n",
       "       [[ 9562.127  ,  6655.463  ]],\n",
       "\n",
       "       [[ 9561.84   ,  2789.0862 ]],\n",
       "\n",
       "       [[10580.419  ,  3505.526  ]],\n",
       "\n",
       "       [[11383.817  ,  5823.6533 ]],\n",
       "\n",
       "       [[12448.344  ,  3502.595  ]],\n",
       "\n",
       "       [[ 6608.931  ,  5981.35   ]],\n",
       "\n",
       "       [[12977.769  ,  2662.3792 ]],\n",
       "\n",
       "       [[ 5722.3843 ,  6734.2734 ]],\n",
       "\n",
       "       [[ 9045.053  ,  9105.584  ]],\n",
       "\n",
       "       [[ 9480.969  ,  3621.3367 ]],\n",
       "\n",
       "       [[ 9730.519  ,  4111.949  ]],\n",
       "\n",
       "       [[12356.031  ,  2817.9192 ]],\n",
       "\n",
       "       [[ 8755.961  ,  4239.32   ]],\n",
       "\n",
       "       [[ 7122.2886 ,  6849.638  ]],\n",
       "\n",
       "       [[ 6752.2603 ,  6839.568  ]],\n",
       "\n",
       "       [[10001.53   ,  5760.845  ]],\n",
       "\n",
       "       [[ 8361.504  ,  6754.1284 ]],\n",
       "\n",
       "       [[10867.919  ,  7056.5337 ]],\n",
       "\n",
       "       [[10596.565  ,  6522.2705 ]],\n",
       "\n",
       "       [[ 8879.457  ,  7958.6978 ]],\n",
       "\n",
       "       [[ 6828.2593 , 10138.21   ]],\n",
       "\n",
       "       [[ 6692.8135 ,  6070.4556 ]],\n",
       "\n",
       "       [[ 6786.128  ,  8155.2485 ]],\n",
       "\n",
       "       [[10739.292  ,  6475.184  ]],\n",
       "\n",
       "       [[ 9445.437  ,  6232.1143 ]],\n",
       "\n",
       "       [[ 7660.4673 ,  7891.47   ]],\n",
       "\n",
       "       [[ 6925.373  ,  6767.8716 ]],\n",
       "\n",
       "       [[ 9206.776  ,  6799.7476 ]],\n",
       "\n",
       "       [[11061.289  ,  6026.149  ]],\n",
       "\n",
       "       [[10791.294  ,  7390.6924 ]],\n",
       "\n",
       "       [[ 7575.6978 ,  4980.6855 ]],\n",
       "\n",
       "       [[ 6583.931  ,  9602.616  ]],\n",
       "\n",
       "       [[ 7234.0967 ,  6795.6875 ]],\n",
       "\n",
       "       [[ 5592.7407 ,  8680.42   ]],\n",
       "\n",
       "       [[ 7289.694  ,  9486.805  ]],\n",
       "\n",
       "       [[ 5100.3384 , 11828.839  ]],\n",
       "\n",
       "       [[10363.369  ,  8600.569  ]],\n",
       "\n",
       "       [[ 9576.506  ,  6890.231  ]],\n",
       "\n",
       "       [[ 9197.193  ,  9066.86   ]],\n",
       "\n",
       "       [[11403.959  ,  6203.41   ]],\n",
       "\n",
       "       [[ 7103.342  ,  8537.169  ]],\n",
       "\n",
       "       [[ 7741.9585 ,  8503.404  ]],\n",
       "\n",
       "       [[10204.692  ,  5145.0767 ]],\n",
       "\n",
       "       [[ 7779.833  ,  9626.9    ]],\n",
       "\n",
       "       [[ 6873.839  ,  7702.6987 ]],\n",
       "\n",
       "       [[ 9417.589  ,  7138.636  ]],\n",
       "\n",
       "       [[ 7748.524  ,  8902.944  ]],\n",
       "\n",
       "       [[10791.943  ,  5881.077  ]],\n",
       "\n",
       "       [[ 8841.667  ,  7983.3525 ]],\n",
       "\n",
       "       [[ 7096.4478 ,  9515.322  ]],\n",
       "\n",
       "       [[12199.357  ,  6738.6987 ]],\n",
       "\n",
       "       [[ 7823.5225 ,  7154.0293 ]],\n",
       "\n",
       "       [[ 8177.544  ,  6788.173  ]],\n",
       "\n",
       "       [[ 7788.869  ,  8660.696  ]],\n",
       "\n",
       "       [[10879.018  ,  6375.8403 ]],\n",
       "\n",
       "       [[ 8311.602  ,  8890.699  ]],\n",
       "\n",
       "       [[ 5843.9795 ,  6717.5    ]],\n",
       "\n",
       "       [[ 8603.276  ,  8502.655  ]],\n",
       "\n",
       "       [[ 8471.437  ,  8445.759  ]],\n",
       "\n",
       "       [[ 9847.292  ,  6523.9106 ]],\n",
       "\n",
       "       [[ 7736.5854 ,  8567.925  ]],\n",
       "\n",
       "       [[ 7213.635  ,  7211.474  ]],\n",
       "\n",
       "       [[ 7877.87   ,  8065.0967 ]],\n",
       "\n",
       "       [[ 4183.8955 , 11427.334  ]],\n",
       "\n",
       "       [[ 5129.1646 ,  6951.2705 ]],\n",
       "\n",
       "       [[ 9232.332  ,  8866.138  ]],\n",
       "\n",
       "       [[ 8025.4214 ,  8627.683  ]],\n",
       "\n",
       "       [[ 7167.386  ,  8713.446  ]],\n",
       "\n",
       "       [[ 8841.667  ,  7983.3525 ]],\n",
       "\n",
       "       [[ 6591.705  , 10006.095  ]],\n",
       "\n",
       "       [[ 9748.317  ,  9891.651  ]],\n",
       "\n",
       "       [[ 8273.132  ,  8037.9766 ]],\n",
       "\n",
       "       [[ 7445.5396 ,  9309.323  ]],\n",
       "\n",
       "       [[ 6392.3774 ,  6981.2725 ]],\n",
       "\n",
       "       [[ 6396.792  ,  8158.6255 ]],\n",
       "\n",
       "       [[ 6378.003  ,  8229.696  ]],\n",
       "\n",
       "       [[ 7584.196  ,  8455.577  ]],\n",
       "\n",
       "       [[ 5724.1943 ,  7971.983  ]],\n",
       "\n",
       "       [[ 6588.766  ,  7364.2427 ]],\n",
       "\n",
       "       [[ 8562.207  ,  8274.548  ]],\n",
       "\n",
       "       [[ 6072.064  ,  7753.183  ]],\n",
       "\n",
       "       [[ 7360.215  ,  9269.803  ]],\n",
       "\n",
       "       [[ 4621.943  ,  9031.644  ]],\n",
       "\n",
       "       [[ 7138.191  ,  7677.431  ]],\n",
       "\n",
       "       [[ 6958.353  ,  6634.402  ]],\n",
       "\n",
       "       [[ 7277.4487 ,  7922.4556 ]],\n",
       "\n",
       "       [[ 9230.272  ,  8576.99   ]],\n",
       "\n",
       "       [[ 7361.502  ,  8282.587  ]],\n",
       "\n",
       "       [[ 7422.831  ,  7458.191  ]],\n",
       "\n",
       "       [[ 8800.624  ,  6840.1963 ]],\n",
       "\n",
       "       [[ 6891.9585 ,  7648.2603 ]],\n",
       "\n",
       "       [[ 6667.988  ,  7675.7476 ]],\n",
       "\n",
       "       [[ 5887.993  ,  7090.7637 ]],\n",
       "\n",
       "       [[ 3861.4058 ,  9924.873  ]],\n",
       "\n",
       "       [[ 6277.3857 ,  6912.252  ]],\n",
       "\n",
       "       [[ 6027.9233 ,  8397.048  ]],\n",
       "\n",
       "       [[ 6912.708  ,  6956.82   ]],\n",
       "\n",
       "       [[ 8981.939  ,  5577.891  ]],\n",
       "\n",
       "       [[ 3616.3662 ,  9668.164  ]],\n",
       "\n",
       "       [[ 6180.23   ,  5697.1294 ]],\n",
       "\n",
       "       [[10443.998  ,  4320.0913 ]],\n",
       "\n",
       "       [[ 5406.8877 ,  8140.9834 ]],\n",
       "\n",
       "       [[ 5262.2427 ,  7321.601  ]],\n",
       "\n",
       "       [[ 5986.6836 ,  5579.42   ]],\n",
       "\n",
       "       [[ 7094.7935 ,  7797.2183 ]],\n",
       "\n",
       "       [[ 7021.625  ,  6567.273  ]],\n",
       "\n",
       "       [[ 8218.379  ,  5482.193  ]],\n",
       "\n",
       "       [[ 8174.808  ,  5141.2534 ]],\n",
       "\n",
       "       [[ 9940.457  ,  4869.1934 ]],\n",
       "\n",
       "       [[ 7479.747  ,  8174.475  ]],\n",
       "\n",
       "       [[ 3474.1394 , 10615.827  ]],\n",
       "\n",
       "       [[10591.422  ,  4381.9204 ]],\n",
       "\n",
       "       [[ 5663.5757 ,  7784.971  ]],\n",
       "\n",
       "       [[11394.796  ,  2586.1726 ]],\n",
       "\n",
       "       [[ 5410.828  ,  6086.773  ]],\n",
       "\n",
       "       [[ 6577.0503 ,  4533.951  ]],\n",
       "\n",
       "       [[ 8834.236  ,  4022.18   ]],\n",
       "\n",
       "       [[ 9636.582  ,  3128.8782 ]],\n",
       "\n",
       "       [[12042.908  ,  2671.7922 ]],\n",
       "\n",
       "       [[ 6134.3755 ,  6644.249  ]],\n",
       "\n",
       "       [[ 4847.378  ,  8556.251  ]],\n",
       "\n",
       "       [[ 6401.774  ,  6342.1997 ]],\n",
       "\n",
       "       [[10544.87   ,  2130.639  ]],\n",
       "\n",
       "       [[ 9583.263  ,  2904.8816 ]],\n",
       "\n",
       "       [[ 9601.37   ,  2472.4978 ]],\n",
       "\n",
       "       [[ 6788.772  ,  6812.3135 ]],\n",
       "\n",
       "       [[ 9290.067  ,  4768.9985 ]],\n",
       "\n",
       "       [[12346.427  ,  2660.0925 ]],\n",
       "\n",
       "       [[ 8061.599  ,  5445.15   ]],\n",
       "\n",
       "       [[10469.339  ,  3305.641  ]],\n",
       "\n",
       "       [[11039.359  ,  4640.248  ]],\n",
       "\n",
       "       [[ 8808.122  ,  7852.79   ]],\n",
       "\n",
       "       [[ 4815.8687 , 11509.985  ]],\n",
       "\n",
       "       [[ 7401.7495 ,  8594.27   ]],\n",
       "\n",
       "       [[ 5030.0786 , 10198.098  ]],\n",
       "\n",
       "       [[ 8805.554  ,  8069.645  ]],\n",
       "\n",
       "       [[ 3673.887  , 14105.412  ]],\n",
       "\n",
       "       [[ 4796.6206 ,  9607.143  ]],\n",
       "\n",
       "       [[ 3813.7617 , 11359.015  ]],\n",
       "\n",
       "       [[ 5427.067  , 12912.736  ]],\n",
       "\n",
       "       [[ 3640.892  , 11041.379  ]],\n",
       "\n",
       "       [[ 3930.4836 , 13323.624  ]],\n",
       "\n",
       "       [[ 4933.0146 , 12219.14   ]],\n",
       "\n",
       "       [[ 7661.817  ,  6638.2275 ]],\n",
       "\n",
       "       [[ 8465.995  ,  6695.7446 ]],\n",
       "\n",
       "       [[ 5084.7456 , 10824.583  ]],\n",
       "\n",
       "       [[ 7241.101  ,  8041.977  ]],\n",
       "\n",
       "       [[ 4449.279  , 11991.163  ]],\n",
       "\n",
       "       [[ 4262.035  , 12219.057  ]],\n",
       "\n",
       "       [[ 6054.2617 ,  6137.4023 ]],\n",
       "\n",
       "       [[11019.294  ,  3630.2332 ]],\n",
       "\n",
       "       [[11996.887  ,  3360.3914 ]],\n",
       "\n",
       "       [[12739.725  ,  3560.2703 ]],\n",
       "\n",
       "       [[10555.471  ,  6196.523  ]],\n",
       "\n",
       "       [[12409.499  ,  6178.521  ]],\n",
       "\n",
       "       [[12950.077  ,  3756.6545 ]],\n",
       "\n",
       "       [[11681.775  ,  3627.9424 ]],\n",
       "\n",
       "       [[10760.472  ,  6352.9844 ]],\n",
       "\n",
       "       [[ 7245.6426 ,  7588.857  ]],\n",
       "\n",
       "       [[12612.507  ,  4023.4482 ]],\n",
       "\n",
       "       [[13380.799  ,  3478.1213 ]],\n",
       "\n",
       "       [[13186.545  ,  4250.118  ]],\n",
       "\n",
       "       [[ 9497.457  ,  7436.032  ]],\n",
       "\n",
       "       [[11654.892  ,  6272.393  ]],\n",
       "\n",
       "       [[13157.587  ,  3947.5784 ]],\n",
       "\n",
       "       [[ 8760.741  ,  8104.016  ]],\n",
       "\n",
       "       [[11364.838  ,  4165.127  ]],\n",
       "\n",
       "       [[11609.642  ,  6607.1826 ]],\n",
       "\n",
       "       [[11766.187  ,  7119.8936 ]],\n",
       "\n",
       "       [[12832.215  ,  5236.0176 ]],\n",
       "\n",
       "       [[10593.51   ,  8280.123  ]],\n",
       "\n",
       "       [[10144.872  ,  5005.306  ]],\n",
       "\n",
       "       [[ 7483.2725 ,  8402.659  ]],\n",
       "\n",
       "       [[12140.146  ,  6431.64   ]],\n",
       "\n",
       "       [[11817.43   ,  3664.1096 ]],\n",
       "\n",
       "       [[12147.124  ,  4129.428  ]],\n",
       "\n",
       "       [[14330.093  ,  4486.647  ]],\n",
       "\n",
       "       [[ 8762.605  ,  8160.639  ]],\n",
       "\n",
       "       [[12902.41   ,  3536.328  ]],\n",
       "\n",
       "       [[10431.832  ,  7873.5156 ]],\n",
       "\n",
       "       [[12222.502  ,  3315.5447 ]],\n",
       "\n",
       "       [[ 8342.252  ,  5655.2827 ]],\n",
       "\n",
       "       [[10513.554  ,  5050.1045 ]],\n",
       "\n",
       "       [[ 9774.875  ,  8233.685  ]],\n",
       "\n",
       "       [[12544.122  ,  6681.68   ]],\n",
       "\n",
       "       [[11088.154  ,  6926.888  ]],\n",
       "\n",
       "       [[10762.309  ,  8502.377  ]],\n",
       "\n",
       "       [[ 7155.357  ,  8194.166  ]],\n",
       "\n",
       "       [[13505.2295 ,  6867.053  ]],\n",
       "\n",
       "       [[ 9179.916  ,  8030.5747 ]],\n",
       "\n",
       "       [[12941.152  ,  3788.6125 ]],\n",
       "\n",
       "       [[ 8760.741  ,  8104.016  ]],\n",
       "\n",
       "       [[12952.123  ,  4188.874  ]],\n",
       "\n",
       "       [[ 8980.451  ,  8584.488  ]],\n",
       "\n",
       "       [[ 8883.983  ,  7775.614  ]],\n",
       "\n",
       "       [[11162.465  ,  4276.782  ]],\n",
       "\n",
       "       [[11355.484  ,  2359.5898 ]],\n",
       "\n",
       "       [[ 8785.199  ,  6434.9985 ]],\n",
       "\n",
       "       [[11422.12   ,  4233.571  ]],\n",
       "\n",
       "       [[12160.021  ,  5422.6987 ]],\n",
       "\n",
       "       [[13233.124  ,  4779.2466 ]],\n",
       "\n",
       "       [[14177.807  ,  5339.4243 ]],\n",
       "\n",
       "       [[ 9871.925  ,  5469.0093 ]],\n",
       "\n",
       "       [[12513.49   ,  3136.3547 ]],\n",
       "\n",
       "       [[ 9387.835  ,  8044.5884 ]],\n",
       "\n",
       "       [[12610.737  ,  4578.8765 ]],\n",
       "\n",
       "       [[ 8046.7593 ,  6307.5464 ]],\n",
       "\n",
       "       [[ 2038.0083 , 10746.923  ]],\n",
       "\n",
       "       [[ 2492.1833 , 11847.717  ]],\n",
       "\n",
       "       [[  901.07697, 12095.139  ]],\n",
       "\n",
       "       [[ 1232.8374 , 11759.651  ]],\n",
       "\n",
       "       [[ 8702.75   ,  9271.519  ]],\n",
       "\n",
       "       [[ 2044.9829 , 10328.808  ]],\n",
       "\n",
       "       [[ 2940.709  ,  9897.213  ]],\n",
       "\n",
       "       [[ 6926.485  ,  6682.3765 ]],\n",
       "\n",
       "       [[ 3844.6719 , 10351.245  ]],\n",
       "\n",
       "       [[ 5335.13   ,  8760.6    ]],\n",
       "\n",
       "       [[  967.9571 , 12745.66   ]],\n",
       "\n",
       "       [[ 1352.9155 , 10682.134  ]],\n",
       "\n",
       "       [[ 3966.5542 ,  9533.234  ]],\n",
       "\n",
       "       [[ 1079.9905 , 12642.934  ]],\n",
       "\n",
       "       [[ 6046.5903 ,  8215.382  ]],\n",
       "\n",
       "       [[ 8132.023  ,  7223.2856 ]],\n",
       "\n",
       "       [[ 3293.676  , 12278.693  ]],\n",
       "\n",
       "       [[ 6448.5537 ,  8123.591  ]],\n",
       "\n",
       "       [[ 3656.8152 , 10202.882  ]]], dtype=float32)"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdense_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(228, 1, 2), dtype=float32, numpy=\n",
       "array([[[13523.,  2841.]],\n",
       "\n",
       "       [[13448.,  3137.]],\n",
       "\n",
       "       [[13473.,  6629.]],\n",
       "\n",
       "       [[ 8419.,  1324.]],\n",
       "\n",
       "       [[11051.,  4654.]],\n",
       "\n",
       "       [[11434.,  5891.]],\n",
       "\n",
       "       [[12366.,  2146.]],\n",
       "\n",
       "       [[ 7555.,  6669.]],\n",
       "\n",
       "       [[13418.,  2734.]],\n",
       "\n",
       "       [[ 6256.,  6279.]],\n",
       "\n",
       "       [[ 8269.,  6972.]],\n",
       "\n",
       "       [[10773.,  5261.]],\n",
       "\n",
       "       [[ 9070.,  3554.]],\n",
       "\n",
       "       [[13003.,  2724.]],\n",
       "\n",
       "       [[10349.,  3280.]],\n",
       "\n",
       "       [[ 7250.,  4710.]],\n",
       "\n",
       "       [[  311.,   649.]],\n",
       "\n",
       "       [[12295.,  2626.]],\n",
       "\n",
       "       [[ 7756.,  2449.]],\n",
       "\n",
       "       [[11981.,  6662.]],\n",
       "\n",
       "       [[10445.,  4762.]],\n",
       "\n",
       "       [[14486., 14291.]],\n",
       "\n",
       "       [[ 7023., 11402.]],\n",
       "\n",
       "       [[ 3132.,  7574.]],\n",
       "\n",
       "       [[ 3688.,  9894.]],\n",
       "\n",
       "       [[12718.,  6216.]],\n",
       "\n",
       "       [[11047.,  5058.]],\n",
       "\n",
       "       [[ 3775., 11105.]],\n",
       "\n",
       "       [[ 7417.,  9750.]],\n",
       "\n",
       "       [[13827., 13452.]],\n",
       "\n",
       "       [[11854.,  3146.]],\n",
       "\n",
       "       [[14258., 14400.]],\n",
       "\n",
       "       [[ 9847.,  4249.]],\n",
       "\n",
       "       [[ 4881.,  8281.]],\n",
       "\n",
       "       [[ 9344.,  7024.]],\n",
       "\n",
       "       [[ 6589., 12618.]],\n",
       "\n",
       "       [[ 4779.,  7333.]],\n",
       "\n",
       "       [[ 3744., 13560.]],\n",
       "\n",
       "       [[ 8495.,  8372.]],\n",
       "\n",
       "       [[11403.,  8168.]],\n",
       "\n",
       "       [[ 7653.,  7577.]],\n",
       "\n",
       "       [[14147.,  5290.]],\n",
       "\n",
       "       [[ 7391.,  7697.]],\n",
       "\n",
       "       [[ 7727.,  7406.]],\n",
       "\n",
       "       [[13568.,  5235.]],\n",
       "\n",
       "       [[ 6961.,  6604.]],\n",
       "\n",
       "       [[ 7921.,  8151.]],\n",
       "\n",
       "       [[13222.,  5087.]],\n",
       "\n",
       "       [[ 7278.,  7866.]],\n",
       "\n",
       "       [[12585.,  4815.]],\n",
       "\n",
       "       [[14277., 14223.]],\n",
       "\n",
       "       [[ 5328.,  8592.]],\n",
       "\n",
       "       [[11846.,  7409.]],\n",
       "\n",
       "       [[ 8783.,  8430.]],\n",
       "\n",
       "       [[ 7860.,  7913.]],\n",
       "\n",
       "       [[ 7057.,  7091.]],\n",
       "\n",
       "       [[13678.,  6716.]],\n",
       "\n",
       "       [[ 7792.,  7338.]],\n",
       "\n",
       "       [[ 8853.,  4351.]],\n",
       "\n",
       "       [[ 8332.,  8002.]],\n",
       "\n",
       "       [[ 8305.,  7801.]],\n",
       "\n",
       "       [[10312.,  9894.]],\n",
       "\n",
       "       [[13360., 12978.]],\n",
       "\n",
       "       [[ 7331.,  7704.]],\n",
       "\n",
       "       [[ 7754.,  7492.]],\n",
       "\n",
       "       [[ 1601., 10401.]],\n",
       "\n",
       "       [[ 6078.,  7360.]],\n",
       "\n",
       "       [[ 6578.,  7624.]],\n",
       "\n",
       "       [[ 7330.,  8843.]],\n",
       "\n",
       "       [[ 7637.,  8107.]],\n",
       "\n",
       "       [[14277., 14223.]],\n",
       "\n",
       "       [[ 4948.,  8343.]],\n",
       "\n",
       "       [[13108., 12410.]],\n",
       "\n",
       "       [[ 8191.,  7829.]],\n",
       "\n",
       "       [[13935., 13540.]],\n",
       "\n",
       "       [[ 4337.,  7402.]],\n",
       "\n",
       "       [[ 6554.,  8341.]],\n",
       "\n",
       "       [[ 7709.,  7611.]],\n",
       "\n",
       "       [[ 6738., 11404.]],\n",
       "\n",
       "       [[ 7249.,  6655.]],\n",
       "\n",
       "       [[ 7072.,  6743.]],\n",
       "\n",
       "       [[ 7554.,  7434.]],\n",
       "\n",
       "       [[ 8019.,  7209.]],\n",
       "\n",
       "       [[ 6836.,  8474.]],\n",
       "\n",
       "       [[ 3016., 11169.]],\n",
       "\n",
       "       [[ 6185.,  6443.]],\n",
       "\n",
       "       [[  351.,   293.]],\n",
       "\n",
       "       [[ 7649.,  6894.]],\n",
       "\n",
       "       [[ 9812.,  9628.]],\n",
       "\n",
       "       [[ 7385.,  6771.]],\n",
       "\n",
       "       [[ 7718.,  7656.]],\n",
       "\n",
       "       [[12883.,  5075.]],\n",
       "\n",
       "       [[ 1766.,  2759.]],\n",
       "\n",
       "       [[ 6560.,  8622.]],\n",
       "\n",
       "       [[ 8116.,  8072.]],\n",
       "\n",
       "       [[ 2462.,  8258.]],\n",
       "\n",
       "       [[ 3655.,  7580.]],\n",
       "\n",
       "       [[ 1296.,  1975.]],\n",
       "\n",
       "       [[ 8208., 10171.]],\n",
       "\n",
       "       [[ 7845.,  3628.]],\n",
       "\n",
       "       [[ 2766.,  7527.]],\n",
       "\n",
       "       [[ 4757.,  7141.]],\n",
       "\n",
       "       [[12558.,  4965.]],\n",
       "\n",
       "       [[ 2446.,  7218.]],\n",
       "\n",
       "       [[ 5189.,  8563.]],\n",
       "\n",
       "       [[ 8503.,  2424.]],\n",
       "\n",
       "       [[ 8461.,  7732.]],\n",
       "\n",
       "       [[  560.,   361.]],\n",
       "\n",
       "       [[ 9397.,  5682.]],\n",
       "\n",
       "       [[ 9302.,  5675.]],\n",
       "\n",
       "       [[11185.,  3285.]],\n",
       "\n",
       "       [[ 4629.,  7184.]],\n",
       "\n",
       "       [[ 1869., 12950.]],\n",
       "\n",
       "       [[13835.,  4842.]],\n",
       "\n",
       "       [[ 4214.,  3810.]],\n",
       "\n",
       "       [[11995.,  2523.]],\n",
       "\n",
       "       [[ 1511.,  7418.]],\n",
       "\n",
       "       [[ 8338.,  2492.]],\n",
       "\n",
       "       [[11037.,  1721.]],\n",
       "\n",
       "       [[ 6258.,  1138.]],\n",
       "\n",
       "       [[12186.,  2202.]],\n",
       "\n",
       "       [[ 6537.,  6642.]],\n",
       "\n",
       "       [[  757.,  7369.]],\n",
       "\n",
       "       [[ 5602.,  6100.]],\n",
       "\n",
       "       [[11563.,  1591.]],\n",
       "\n",
       "       [[11285.,  1635.]],\n",
       "\n",
       "       [[11640.,  1426.]],\n",
       "\n",
       "       [[  221.,   471.]],\n",
       "\n",
       "       [[ 7659.,  1139.]],\n",
       "\n",
       "       [[12537.,  2128.]],\n",
       "\n",
       "       [[ 7586.,  2694.]],\n",
       "\n",
       "       [[ 8943.,  1531.]],\n",
       "\n",
       "       [[ 8309.,  3595.]],\n",
       "\n",
       "       [[11500.,  3886.]],\n",
       "\n",
       "       [[ 2728., 11397.]],\n",
       "\n",
       "       [[ 6089., 13203.]],\n",
       "\n",
       "       [[ 3312., 10823.]],\n",
       "\n",
       "       [[14486., 14511.]],\n",
       "\n",
       "       [[ 1859., 12454.]],\n",
       "\n",
       "       [[ 1726., 11521.]],\n",
       "\n",
       "       [[ 3924., 13354.]],\n",
       "\n",
       "       [[ 1937., 12242.]],\n",
       "\n",
       "       [[ 3491., 10883.]],\n",
       "\n",
       "       [[ 1575., 12095.]],\n",
       "\n",
       "       [[ 1185., 10706.]],\n",
       "\n",
       "       [[11451.,  7179.]],\n",
       "\n",
       "       [[13944., 12794.]],\n",
       "\n",
       "       [[ 8002., 13607.]],\n",
       "\n",
       "       [[12371., 13097.]],\n",
       "\n",
       "       [[ 1929., 12172.]],\n",
       "\n",
       "       [[ 3645., 13762.]],\n",
       "\n",
       "       [[ 7263.,  6965.]],\n",
       "\n",
       "       [[13225.,  4199.]],\n",
       "\n",
       "       [[11797.,  1800.]],\n",
       "\n",
       "       [[12362.,  2269.]],\n",
       "\n",
       "       [[12887.,  3401.]],\n",
       "\n",
       "       [[13834.,  4231.]],\n",
       "\n",
       "       [[12276.,  2080.]],\n",
       "\n",
       "       [[12709.,  2618.]],\n",
       "\n",
       "       [[11224.,  1713.]],\n",
       "\n",
       "       [[ 7162.,  7576.]],\n",
       "\n",
       "       [[13275.,  5395.]],\n",
       "\n",
       "       [[11987.,  2030.]],\n",
       "\n",
       "       [[12834.,  2854.]],\n",
       "\n",
       "       [[10272.,  1737.]],\n",
       "\n",
       "       [[13202.,  4578.]],\n",
       "\n",
       "       [[13487.,  4125.]],\n",
       "\n",
       "       [[14147., 14401.]],\n",
       "\n",
       "       [[10943.,  5195.]],\n",
       "\n",
       "       [[13886.,  9439.]],\n",
       "\n",
       "       [[12581.,  3227.]],\n",
       "\n",
       "       [[13157.,  4898.]],\n",
       "\n",
       "       [[ 7788.,  7144.]],\n",
       "\n",
       "       [[ 9569.,  7076.]],\n",
       "\n",
       "       [[ 8196.,  7525.]],\n",
       "\n",
       "       [[10903.,  4410.]],\n",
       "\n",
       "       [[13566.,  4088.]],\n",
       "\n",
       "       [[11551.,  3007.]],\n",
       "\n",
       "       [[13374.,  3298.]],\n",
       "\n",
       "       [[14237., 14579.]],\n",
       "\n",
       "       [[12436.,  1843.]],\n",
       "\n",
       "       [[12784.,  7408.]],\n",
       "\n",
       "       [[11584.,  1743.]],\n",
       "\n",
       "       [[10422.,  7466.]],\n",
       "\n",
       "       [[13350.,  3678.]],\n",
       "\n",
       "       [[11355.,  6532.]],\n",
       "\n",
       "       [[13268.,  3752.]],\n",
       "\n",
       "       [[13882., 10115.]],\n",
       "\n",
       "       [[13565.,  4426.]],\n",
       "\n",
       "       [[ 3796.,  8578.]],\n",
       "\n",
       "       [[14104.,  4865.]],\n",
       "\n",
       "       [[ 7906.,  8146.]],\n",
       "\n",
       "       [[12506.,  2885.]],\n",
       "\n",
       "       [[14147., 14401.]],\n",
       "\n",
       "       [[12054.,  2779.]],\n",
       "\n",
       "       [[14147., 12784.]],\n",
       "\n",
       "       [[ 8216.,  8452.]],\n",
       "\n",
       "       [[12817.,  2271.]],\n",
       "\n",
       "       [[10965.,  1580.]],\n",
       "\n",
       "       [[14340., 14391.]],\n",
       "\n",
       "       [[10385.,  5591.]],\n",
       "\n",
       "       [[12892.,  1991.]],\n",
       "\n",
       "       [[13145.,  2050.]],\n",
       "\n",
       "       [[13197.,  3896.]],\n",
       "\n",
       "       [[12524.,  7199.]],\n",
       "\n",
       "       [[11132.,  1519.]],\n",
       "\n",
       "       [[ 8597.,  7852.]],\n",
       "\n",
       "       [[13164.,  4566.]],\n",
       "\n",
       "       [[ 7499.,  7989.]],\n",
       "\n",
       "       [[ 1689., 11046.]],\n",
       "\n",
       "       [[ 1701., 12879.]],\n",
       "\n",
       "       [[ 2660., 13419.]],\n",
       "\n",
       "       [[ 1507., 11145.]],\n",
       "\n",
       "       [[ 7469.,  3808.]],\n",
       "\n",
       "       [[ 2888., 11098.]],\n",
       "\n",
       "       [[ 4117.,  9828.]],\n",
       "\n",
       "       [[  560.,   581.]],\n",
       "\n",
       "       [[ 1534.,  8762.]],\n",
       "\n",
       "       [[ 2374.,  9869.]],\n",
       "\n",
       "       [[ 1836., 12199.]],\n",
       "\n",
       "       [[ 1686., 10358.]],\n",
       "\n",
       "       [[ 4126., 12791.]],\n",
       "\n",
       "       [[ 2853., 13321.]],\n",
       "\n",
       "       [[ 4485.,  9464.]],\n",
       "\n",
       "       [[ 7358.,  3872.]],\n",
       "\n",
       "       [[ 3153., 13180.]],\n",
       "\n",
       "       [[ 7904.,  8704.]],\n",
       "\n",
       "       [[ 2620., 12212.]]], dtype=float32)>"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_hold[0][:,:,-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(228, 1, 2), dtype=float32, numpy=\n",
       "array([[[13003.,  2724.]],\n",
       "\n",
       "       [[ 8419.,  1324.]],\n",
       "\n",
       "       [[ 7555.,  6669.]],\n",
       "\n",
       "       [[10773.,  5261.]],\n",
       "\n",
       "       [[13418.,  2734.]],\n",
       "\n",
       "       [[13523.,  2841.]],\n",
       "\n",
       "       [[ 9070.,  3554.]],\n",
       "\n",
       "       [[ 7250.,  4710.]],\n",
       "\n",
       "       [[11434.,  5891.]],\n",
       "\n",
       "       [[ 7756.,  2449.]],\n",
       "\n",
       "       [[ 6256.,  6279.]],\n",
       "\n",
       "       [[10349.,  3280.]],\n",
       "\n",
       "       [[13448.,  3137.]],\n",
       "\n",
       "       [[12366.,  2146.]],\n",
       "\n",
       "       [[12295.,  2626.]],\n",
       "\n",
       "       [[ 8269.,  6972.]],\n",
       "\n",
       "       [[11051.,  4654.]],\n",
       "\n",
       "       [[13473.,  6629.]],\n",
       "\n",
       "       [[ 2706., 12710.]],\n",
       "\n",
       "       [[ 4779.,  7333.]],\n",
       "\n",
       "       [[ 7023., 11402.]],\n",
       "\n",
       "       [[11047.,  5058.]],\n",
       "\n",
       "       [[13827., 13452.]],\n",
       "\n",
       "       [[ 9847.,  4249.]],\n",
       "\n",
       "       [[ 3775., 11105.]],\n",
       "\n",
       "       [[10445.,  4762.]],\n",
       "\n",
       "       [[ 4881.,  8281.]],\n",
       "\n",
       "       [[ 9344.,  7024.]],\n",
       "\n",
       "       [[ 3688.,  9894.]],\n",
       "\n",
       "       [[11854.,  3146.]],\n",
       "\n",
       "       [[ 6589., 12618.]],\n",
       "\n",
       "       [[11981.,  6662.]],\n",
       "\n",
       "       [[ 7417.,  9750.]],\n",
       "\n",
       "       [[14258., 14400.]],\n",
       "\n",
       "       [[ 5702.,  5827.]],\n",
       "\n",
       "       [[ 3132.,  7574.]],\n",
       "\n",
       "       [[ 3744., 13560.]],\n",
       "\n",
       "       [[12718.,  6216.]],\n",
       "\n",
       "       [[ 7727.,  7406.]],\n",
       "\n",
       "       [[ 6961.,  6604.]],\n",
       "\n",
       "       [[14147.,  5290.]],\n",
       "\n",
       "       [[12585.,  4815.]],\n",
       "\n",
       "       [[ 7921.,  8151.]],\n",
       "\n",
       "       [[ 7057.,  7091.]],\n",
       "\n",
       "       [[11403.,  8168.]],\n",
       "\n",
       "       [[ 7653.,  7577.]],\n",
       "\n",
       "       [[ 8495.,  8372.]],\n",
       "\n",
       "       [[13678.,  6716.]],\n",
       "\n",
       "       [[ 8783.,  8430.]],\n",
       "\n",
       "       [[11846.,  7409.]],\n",
       "\n",
       "       [[ 7860.,  7913.]],\n",
       "\n",
       "       [[13568.,  5235.]],\n",
       "\n",
       "       [[13222.,  5087.]],\n",
       "\n",
       "       [[ 5328.,  8592.]],\n",
       "\n",
       "       [[ 7391.,  7697.]],\n",
       "\n",
       "       [[ 7278.,  7866.]],\n",
       "\n",
       "       [[13682.,  7712.]],\n",
       "\n",
       "       [[ 7330.,  8843.]],\n",
       "\n",
       "       [[ 9723.,  4866.]],\n",
       "\n",
       "       [[ 6578.,  7624.]],\n",
       "\n",
       "       [[ 8191.,  7829.]],\n",
       "\n",
       "       [[ 4948.,  8343.]],\n",
       "\n",
       "       [[ 7792.,  7338.]],\n",
       "\n",
       "       [[ 8332.,  8002.]],\n",
       "\n",
       "       [[ 7637.,  8107.]],\n",
       "\n",
       "       [[ 6078.,  7360.]],\n",
       "\n",
       "       [[ 4337.,  7402.]],\n",
       "\n",
       "       [[ 8305.,  7801.]],\n",
       "\n",
       "       [[ 1601., 10401.]],\n",
       "\n",
       "       [[10312.,  9894.]],\n",
       "\n",
       "       [[ 7331.,  7704.]],\n",
       "\n",
       "       [[13360., 12978.]],\n",
       "\n",
       "       [[ 7754.,  7492.]],\n",
       "\n",
       "       [[13108., 12410.]],\n",
       "\n",
       "       [[ 8853.,  4351.]],\n",
       "\n",
       "       [[13935., 13540.]],\n",
       "\n",
       "       [[ 6185.,  6443.]],\n",
       "\n",
       "       [[ 7385.,  6771.]],\n",
       "\n",
       "       [[ 6586., 13271.]],\n",
       "\n",
       "       [[12883.,  5075.]],\n",
       "\n",
       "       [[ 7649.,  6894.]],\n",
       "\n",
       "       [[ 8019.,  7209.]],\n",
       "\n",
       "       [[ 7249.,  6655.]],\n",
       "\n",
       "       [[ 7709.,  7611.]],\n",
       "\n",
       "       [[ 9812.,  9628.]],\n",
       "\n",
       "       [[ 7718.,  7656.]],\n",
       "\n",
       "       [[ 6560.,  8622.]],\n",
       "\n",
       "       [[ 8116.,  8072.]],\n",
       "\n",
       "       [[ 6738., 11404.]],\n",
       "\n",
       "       [[ 7072.,  6743.]],\n",
       "\n",
       "       [[ 6836.,  8474.]],\n",
       "\n",
       "       [[ 1766.,  2759.]],\n",
       "\n",
       "       [[ 3016., 11169.]],\n",
       "\n",
       "       [[ 6554.,  8341.]],\n",
       "\n",
       "       [[ 7554.,  7434.]],\n",
       "\n",
       "       [[ 7845.,  3628.]],\n",
       "\n",
       "       [[ 8617.,  4790.]],\n",
       "\n",
       "       [[ 9302.,  5675.]],\n",
       "\n",
       "       [[ 8503.,  2424.]],\n",
       "\n",
       "       [[12558.,  4965.]],\n",
       "\n",
       "       [[ 8461.,  7732.]],\n",
       "\n",
       "       [[ 1869., 12950.]],\n",
       "\n",
       "       [[ 2766.,  7527.]],\n",
       "\n",
       "       [[13835.,  4842.]],\n",
       "\n",
       "       [[ 2462.,  8258.]],\n",
       "\n",
       "       [[ 3655.,  7580.]],\n",
       "\n",
       "       [[ 9397.,  5682.]],\n",
       "\n",
       "       [[ 5189.,  8563.]],\n",
       "\n",
       "       [[11185.,  3285.]],\n",
       "\n",
       "       [[ 2446.,  7218.]],\n",
       "\n",
       "       [[ 1296.,  1975.]],\n",
       "\n",
       "       [[ 8208., 10171.]],\n",
       "\n",
       "       [[ 4629.,  7184.]],\n",
       "\n",
       "       [[ 4757.,  7141.]],\n",
       "\n",
       "       [[  965.,  2013.]],\n",
       "\n",
       "       [[11563.,  1591.]],\n",
       "\n",
       "       [[ 4214.,  3810.]],\n",
       "\n",
       "       [[ 7586.,  2694.]],\n",
       "\n",
       "       [[ 5602.,  6100.]],\n",
       "\n",
       "       [[11640.,  1426.]],\n",
       "\n",
       "       [[ 6258.,  1138.]],\n",
       "\n",
       "       [[ 1511.,  7418.]],\n",
       "\n",
       "       [[ 6537.,  6642.]],\n",
       "\n",
       "       [[ 8338.,  2492.]],\n",
       "\n",
       "       [[ 8943.,  1531.]],\n",
       "\n",
       "       [[ 7659.,  1139.]],\n",
       "\n",
       "       [[11037.,  1721.]],\n",
       "\n",
       "       [[ 8309.,  3595.]],\n",
       "\n",
       "       [[12186.,  2202.]],\n",
       "\n",
       "       [[11995.,  2523.]],\n",
       "\n",
       "       [[  757.,  7369.]],\n",
       "\n",
       "       [[11285.,  1635.]],\n",
       "\n",
       "       [[12537.,  2128.]],\n",
       "\n",
       "       [[ 6089., 13203.]],\n",
       "\n",
       "       [[13944., 12794.]],\n",
       "\n",
       "       [[11451.,  7179.]],\n",
       "\n",
       "       [[ 1726., 11521.]],\n",
       "\n",
       "       [[ 3491., 10883.]],\n",
       "\n",
       "       [[ 1575., 12095.]],\n",
       "\n",
       "       [[ 3645., 13762.]],\n",
       "\n",
       "       [[ 1185., 10706.]],\n",
       "\n",
       "       [[ 8002., 13607.]],\n",
       "\n",
       "       [[ 1859., 12454.]],\n",
       "\n",
       "       [[ 3924., 13354.]],\n",
       "\n",
       "       [[12371., 13097.]],\n",
       "\n",
       "       [[13965., 10946.]],\n",
       "\n",
       "       [[11500.,  3886.]],\n",
       "\n",
       "       [[ 2728., 11397.]],\n",
       "\n",
       "       [[ 7263.,  6965.]],\n",
       "\n",
       "       [[ 3312., 10823.]],\n",
       "\n",
       "       [[ 1937., 12242.]],\n",
       "\n",
       "       [[ 1929., 12172.]],\n",
       "\n",
       "       [[13202.,  4578.]],\n",
       "\n",
       "       [[13275.,  5395.]],\n",
       "\n",
       "       [[12276.,  2080.]],\n",
       "\n",
       "       [[10272.,  1737.]],\n",
       "\n",
       "       [[11797.,  1800.]],\n",
       "\n",
       "       [[11987.,  2030.]],\n",
       "\n",
       "       [[13886.,  9439.]],\n",
       "\n",
       "       [[12709.,  2618.]],\n",
       "\n",
       "       [[ 7123.,  7504.]],\n",
       "\n",
       "       [[12834.,  2854.]],\n",
       "\n",
       "       [[13834.,  4231.]],\n",
       "\n",
       "       [[13487.,  4125.]],\n",
       "\n",
       "       [[ 7162.,  7576.]],\n",
       "\n",
       "       [[11224.,  1713.]],\n",
       "\n",
       "       [[13225.,  4199.]],\n",
       "\n",
       "       [[10943.,  5195.]],\n",
       "\n",
       "       [[12362.,  2269.]],\n",
       "\n",
       "       [[12581.,  3227.]],\n",
       "\n",
       "       [[12887.,  3401.]],\n",
       "\n",
       "       [[13350.,  3678.]],\n",
       "\n",
       "       [[ 3796.,  8578.]],\n",
       "\n",
       "       [[12436.,  1843.]],\n",
       "\n",
       "       [[ 8552., 11594.]],\n",
       "\n",
       "       [[13157.,  4898.]],\n",
       "\n",
       "       [[11551.,  3007.]],\n",
       "\n",
       "       [[13882., 10115.]],\n",
       "\n",
       "       [[13565.,  4426.]],\n",
       "\n",
       "       [[ 9569.,  7076.]],\n",
       "\n",
       "       [[11584.,  1743.]],\n",
       "\n",
       "       [[ 7788.,  7144.]],\n",
       "\n",
       "       [[13566.,  4088.]],\n",
       "\n",
       "       [[13268.,  3752.]],\n",
       "\n",
       "       [[10422.,  7466.]],\n",
       "\n",
       "       [[ 8196.,  7525.]],\n",
       "\n",
       "       [[12784.,  7408.]],\n",
       "\n",
       "       [[13374.,  3298.]],\n",
       "\n",
       "       [[10903.,  4410.]],\n",
       "\n",
       "       [[11355.,  6532.]],\n",
       "\n",
       "       [[12524.,  7199.]],\n",
       "\n",
       "       [[ 8216.,  8452.]],\n",
       "\n",
       "       [[11132.,  1519.]],\n",
       "\n",
       "       [[10385.,  5591.]],\n",
       "\n",
       "       [[12892.,  1991.]],\n",
       "\n",
       "       [[12054.,  2779.]],\n",
       "\n",
       "       [[ 7499.,  7989.]],\n",
       "\n",
       "       [[14104.,  4865.]],\n",
       "\n",
       "       [[14147., 12784.]],\n",
       "\n",
       "       [[10965.,  1580.]],\n",
       "\n",
       "       [[12506.,  2885.]],\n",
       "\n",
       "       [[12817.,  2271.]],\n",
       "\n",
       "       [[13164.,  4566.]],\n",
       "\n",
       "       [[13145.,  2050.]],\n",
       "\n",
       "       [[ 7906.,  8146.]],\n",
       "\n",
       "       [[13197.,  3896.]],\n",
       "\n",
       "       [[ 9836.,  9628.]],\n",
       "\n",
       "       [[14340., 14391.]],\n",
       "\n",
       "       [[ 8597.,  7852.]],\n",
       "\n",
       "       [[ 4126., 12791.]],\n",
       "\n",
       "       [[ 2374.,  9869.]],\n",
       "\n",
       "       [[ 1686., 10358.]],\n",
       "\n",
       "       [[ 3153., 13180.]],\n",
       "\n",
       "       [[ 4485.,  9464.]],\n",
       "\n",
       "       [[ 1836., 12199.]],\n",
       "\n",
       "       [[ 1701., 12879.]],\n",
       "\n",
       "       [[ 2888., 11098.]],\n",
       "\n",
       "       [[ 4117.,  9828.]],\n",
       "\n",
       "       [[ 1689., 11046.]],\n",
       "\n",
       "       [[ 2853., 13321.]],\n",
       "\n",
       "       [[ 2620., 12212.]],\n",
       "\n",
       "       [[ 2660., 13419.]],\n",
       "\n",
       "       [[ 1507., 11145.]],\n",
       "\n",
       "       [[ 7904.,  8704.]],\n",
       "\n",
       "       [[ 4830., 12272.]],\n",
       "\n",
       "       [[ 1534.,  8762.]],\n",
       "\n",
       "       [[ 7358.,  3872.]],\n",
       "\n",
       "       [[ 7469.,  3808.]]], dtype=float32)>"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_hold[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
