{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from riotwatcher import LolWatcher, ApiError\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from time import sleep\n",
    "\n",
    "with open('data/api-key.txt', 'r') as api:\n",
    "    API_KEY = api.read()\n",
    "lol_watcher = LolWatcher(API_KEY)\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4)\"+\n",
    "                    \" AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.135\"+\n",
    "                    \" Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Accept-Charset\": \"application/x-www-form-urlencoded; charset=UTF-8\",\n",
    "    \"Origin\": \"https://developer.riotgames.com\",\n",
    "    \"X-Riot-Token\": API_KEY\n",
    "}\n",
    "\n",
    "CHAMP_ID = 64\n",
    "TESTMATCH = \"4748107995\"\n",
    "\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "#import IPython\n",
    "#import IPython.display\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_dict = pickle.load(open(\"data/timeline-di.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gameids = list(dfs_dict.keys())\n",
    "gameids.sort()\n",
    "train_ids = gameids[0:int(len(gameids)*.8)]\n",
    "val_ids = gameids[int(len(gameids)*.8):int(len(gameids)*.9)]\n",
    "test_ids = gameids[int(len(gameids)*.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>0currentGold</th>\n",
       "      <th>1currentGold</th>\n",
       "      <th>2currentGold</th>\n",
       "      <th>3currentGold</th>\n",
       "      <th>4currentGold</th>\n",
       "      <th>5currentGold</th>\n",
       "      <th>6currentGold</th>\n",
       "      <th>7currentGold</th>\n",
       "      <th>8currentGold</th>\n",
       "      <th>...</th>\n",
       "      <th>1y</th>\n",
       "      <th>2y</th>\n",
       "      <th>3y</th>\n",
       "      <th>4y</th>\n",
       "      <th>6y</th>\n",
       "      <th>7y</th>\n",
       "      <th>8y</th>\n",
       "      <th>9y</th>\n",
       "      <th>player_x</th>\n",
       "      <th>player_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>361.0</td>\n",
       "      <td>293.0</td>\n",
       "      <td>471.0</td>\n",
       "      <td>649.0</td>\n",
       "      <td>14291.0</td>\n",
       "      <td>14223.0</td>\n",
       "      <td>14401.0</td>\n",
       "      <td>14579.0</td>\n",
       "      <td>14486.0</td>\n",
       "      <td>14511.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60021.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5918.0</td>\n",
       "      <td>7122.0</td>\n",
       "      <td>1061.0</td>\n",
       "      <td>5697.0</td>\n",
       "      <td>6837.0</td>\n",
       "      <td>6371.0</td>\n",
       "      <td>4256.0</td>\n",
       "      <td>5165.0</td>\n",
       "      <td>13192.0</td>\n",
       "      <td>14166.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>120038.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2128.0</td>\n",
       "      <td>6876.0</td>\n",
       "      <td>2722.0</td>\n",
       "      <td>2688.0</td>\n",
       "      <td>7753.0</td>\n",
       "      <td>7563.0</td>\n",
       "      <td>3038.0</td>\n",
       "      <td>2617.0</td>\n",
       "      <td>4234.0</td>\n",
       "      <td>13602.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>180049.0</td>\n",
       "      <td>455.0</td>\n",
       "      <td>453.0</td>\n",
       "      <td>469.0</td>\n",
       "      <td>614.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>381.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5054.0</td>\n",
       "      <td>6181.0</td>\n",
       "      <td>4045.0</td>\n",
       "      <td>3444.0</td>\n",
       "      <td>5250.0</td>\n",
       "      <td>7136.0</td>\n",
       "      <td>5056.0</td>\n",
       "      <td>7300.0</td>\n",
       "      <td>3871.0</td>\n",
       "      <td>13475.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>240068.0</td>\n",
       "      <td>781.0</td>\n",
       "      <td>947.0</td>\n",
       "      <td>717.0</td>\n",
       "      <td>1358.0</td>\n",
       "      <td>339.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5411.0</td>\n",
       "      <td>7268.0</td>\n",
       "      <td>2113.0</td>\n",
       "      <td>2290.0</td>\n",
       "      <td>13996.0</td>\n",
       "      <td>8934.0</td>\n",
       "      <td>8532.0</td>\n",
       "      <td>2420.0</td>\n",
       "      <td>4153.0</td>\n",
       "      <td>13654.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>300080.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>1232.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>1830.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>526.0</td>\n",
       "      <td>495.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8268.0</td>\n",
       "      <td>6871.0</td>\n",
       "      <td>1171.0</td>\n",
       "      <td>1372.0</td>\n",
       "      <td>9862.0</td>\n",
       "      <td>7741.0</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>1687.0</td>\n",
       "      <td>4254.0</td>\n",
       "      <td>13656.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>360102.0</td>\n",
       "      <td>418.0</td>\n",
       "      <td>1314.0</td>\n",
       "      <td>587.0</td>\n",
       "      <td>965.0</td>\n",
       "      <td>793.0</td>\n",
       "      <td>728.0</td>\n",
       "      <td>821.0</td>\n",
       "      <td>859.0</td>\n",
       "      <td>533.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13841.0</td>\n",
       "      <td>13390.0</td>\n",
       "      <td>520.0</td>\n",
       "      <td>1237.0</td>\n",
       "      <td>4689.0</td>\n",
       "      <td>5565.0</td>\n",
       "      <td>1573.0</td>\n",
       "      <td>1547.0</td>\n",
       "      <td>4405.0</td>\n",
       "      <td>14040.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>420109.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>346.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>331.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>492.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3688.0</td>\n",
       "      <td>7655.0</td>\n",
       "      <td>2539.0</td>\n",
       "      <td>2698.0</td>\n",
       "      <td>13300.0</td>\n",
       "      <td>11346.0</td>\n",
       "      <td>2695.0</td>\n",
       "      <td>3577.0</td>\n",
       "      <td>3241.0</td>\n",
       "      <td>13286.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>480142.0</td>\n",
       "      <td>383.0</td>\n",
       "      <td>802.0</td>\n",
       "      <td>688.0</td>\n",
       "      <td>840.0</td>\n",
       "      <td>430.0</td>\n",
       "      <td>921.0</td>\n",
       "      <td>482.0</td>\n",
       "      <td>429.0</td>\n",
       "      <td>344.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5488.0</td>\n",
       "      <td>5054.0</td>\n",
       "      <td>4249.0</td>\n",
       "      <td>5181.0</td>\n",
       "      <td>10001.0</td>\n",
       "      <td>7477.0</td>\n",
       "      <td>5826.0</td>\n",
       "      <td>5571.0</td>\n",
       "      <td>2137.0</td>\n",
       "      <td>11848.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>540170.0</td>\n",
       "      <td>781.0</td>\n",
       "      <td>1284.0</td>\n",
       "      <td>961.0</td>\n",
       "      <td>1409.0</td>\n",
       "      <td>707.0</td>\n",
       "      <td>1177.0</td>\n",
       "      <td>643.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>545.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7369.0</td>\n",
       "      <td>6941.0</td>\n",
       "      <td>4785.0</td>\n",
       "      <td>7242.0</td>\n",
       "      <td>7701.0</td>\n",
       "      <td>8166.0</td>\n",
       "      <td>7717.0</td>\n",
       "      <td>7742.0</td>\n",
       "      <td>4345.0</td>\n",
       "      <td>13525.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp  0currentGold  1currentGold  2currentGold  3currentGold  \\\n",
       "0        0.0         500.0         500.0         500.0         500.0   \n",
       "1    60021.0           0.0           0.0           0.0           0.0   \n",
       "2   120038.0         126.0         194.0          98.0          63.0   \n",
       "3   180049.0         455.0         453.0         469.0         614.0   \n",
       "4   240068.0         781.0         947.0         717.0        1358.0   \n",
       "5   300080.0          87.0        1232.0         176.0        1830.0   \n",
       "6   360102.0         418.0        1314.0         587.0         965.0   \n",
       "7   420109.0          59.0         346.0         257.0         331.0   \n",
       "8   480142.0         383.0         802.0         688.0         840.0   \n",
       "9   540170.0         781.0        1284.0         961.0        1409.0   \n",
       "\n",
       "   4currentGold  5currentGold  6currentGold  7currentGold  8currentGold  ...  \\\n",
       "0         500.0         500.0         500.0         500.0         500.0  ...   \n",
       "1           0.0           0.0           0.0           0.0          15.0  ...   \n",
       "2          44.0          42.0         121.0          98.0          98.0  ...   \n",
       "3         134.0         311.0         138.0         381.0         171.0  ...   \n",
       "4         339.0         560.0         261.0          79.0          46.0  ...   \n",
       "5         558.0         144.0         526.0         495.0         297.0  ...   \n",
       "6         793.0         728.0         821.0         859.0         533.0  ...   \n",
       "7         192.0         492.0          48.0          49.0          83.0  ...   \n",
       "8         430.0         921.0         482.0         429.0         344.0  ...   \n",
       "9         707.0        1177.0         643.0          55.0         545.0  ...   \n",
       "\n",
       "        1y       2y      3y      4y       6y       7y       8y       9y  \\\n",
       "0    361.0    293.0   471.0   649.0  14291.0  14223.0  14401.0  14579.0   \n",
       "1   5918.0   7122.0  1061.0  5697.0   6837.0   6371.0   4256.0   5165.0   \n",
       "2   2128.0   6876.0  2722.0  2688.0   7753.0   7563.0   3038.0   2617.0   \n",
       "3   5054.0   6181.0  4045.0  3444.0   5250.0   7136.0   5056.0   7300.0   \n",
       "4   5411.0   7268.0  2113.0  2290.0  13996.0   8934.0   8532.0   2420.0   \n",
       "5   8268.0   6871.0  1171.0  1372.0   9862.0   7741.0   1260.0   1687.0   \n",
       "6  13841.0  13390.0   520.0  1237.0   4689.0   5565.0   1573.0   1547.0   \n",
       "7   3688.0   7655.0  2539.0  2698.0  13300.0  11346.0   2695.0   3577.0   \n",
       "8   5488.0   5054.0  4249.0  5181.0  10001.0   7477.0   5826.0   5571.0   \n",
       "9   7369.0   6941.0  4785.0  7242.0   7701.0   8166.0   7717.0   7742.0   \n",
       "\n",
       "   player_x  player_y  \n",
       "0   14486.0   14511.0  \n",
       "1   13192.0   14166.0  \n",
       "2    4234.0   13602.0  \n",
       "3    3871.0   13475.0  \n",
       "4    4153.0   13654.0  \n",
       "5    4254.0   13656.0  \n",
       "6    4405.0   14040.0  \n",
       "7    3241.0   13286.0  \n",
       "8    2137.0   11848.0  \n",
       "9    4345.0   13525.0  \n",
       "\n",
       "[10 rows x 61 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_dict[train_ids[1]][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_ids[0]\n",
    "test0 = dfs_dict[train_ids[0]][0:20].drop([\"timestamp\"],axis=1)\n",
    "test1 = dfs_dict[train_ids[1]][0:20].drop([\"timestamp\"],axis=1)\n",
    "column_indices = {name: i for i, name in enumerate(test0.columns)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0currentGold</th>\n",
       "      <th>1currentGold</th>\n",
       "      <th>2currentGold</th>\n",
       "      <th>3currentGold</th>\n",
       "      <th>4currentGold</th>\n",
       "      <th>5currentGold</th>\n",
       "      <th>6currentGold</th>\n",
       "      <th>7currentGold</th>\n",
       "      <th>8currentGold</th>\n",
       "      <th>9currentGold</th>\n",
       "      <th>...</th>\n",
       "      <th>1y</th>\n",
       "      <th>3y</th>\n",
       "      <th>4y</th>\n",
       "      <th>5y</th>\n",
       "      <th>6y</th>\n",
       "      <th>7y</th>\n",
       "      <th>8y</th>\n",
       "      <th>9y</th>\n",
       "      <th>player_x</th>\n",
       "      <th>player_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>361.0</td>\n",
       "      <td>471.0</td>\n",
       "      <td>649.0</td>\n",
       "      <td>14511.0</td>\n",
       "      <td>14291.0</td>\n",
       "      <td>14223.0</td>\n",
       "      <td>14401.0</td>\n",
       "      <td>14579.0</td>\n",
       "      <td>351.0</td>\n",
       "      <td>293.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>542.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>679.0</td>\n",
       "      <td>13885.0</td>\n",
       "      <td>11463.0</td>\n",
       "      <td>8416.0</td>\n",
       "      <td>7493.0</td>\n",
       "      <td>11457.0</td>\n",
       "      <td>6348.0</td>\n",
       "      <td>9817.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>122.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8042.0</td>\n",
       "      <td>2285.0</td>\n",
       "      <td>2107.0</td>\n",
       "      <td>12344.0</td>\n",
       "      <td>10103.0</td>\n",
       "      <td>7015.0</td>\n",
       "      <td>3101.0</td>\n",
       "      <td>2597.0</td>\n",
       "      <td>6598.0</td>\n",
       "      <td>6970.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>356.0</td>\n",
       "      <td>634.0</td>\n",
       "      <td>486.0</td>\n",
       "      <td>521.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>359.0</td>\n",
       "      <td>540.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>373.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5011.0</td>\n",
       "      <td>3118.0</td>\n",
       "      <td>3266.0</td>\n",
       "      <td>13855.0</td>\n",
       "      <td>4991.0</td>\n",
       "      <td>7337.0</td>\n",
       "      <td>3991.0</td>\n",
       "      <td>4151.0</td>\n",
       "      <td>7235.0</td>\n",
       "      <td>6689.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>128.0</td>\n",
       "      <td>1475.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>485.0</td>\n",
       "      <td>816.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>648.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3263.0</td>\n",
       "      <td>2921.0</td>\n",
       "      <td>2816.0</td>\n",
       "      <td>10840.0</td>\n",
       "      <td>9761.0</td>\n",
       "      <td>9040.0</td>\n",
       "      <td>3937.0</td>\n",
       "      <td>11836.0</td>\n",
       "      <td>7473.0</td>\n",
       "      <td>7689.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>453.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>454.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>413.0</td>\n",
       "      <td>379.0</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7169.0</td>\n",
       "      <td>1556.0</td>\n",
       "      <td>1712.0</td>\n",
       "      <td>12890.0</td>\n",
       "      <td>8567.0</td>\n",
       "      <td>8337.0</td>\n",
       "      <td>3373.0</td>\n",
       "      <td>3711.0</td>\n",
       "      <td>8044.0</td>\n",
       "      <td>7598.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>803.0</td>\n",
       "      <td>630.0</td>\n",
       "      <td>751.0</td>\n",
       "      <td>439.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>527.0</td>\n",
       "      <td>696.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>429.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12602.0</td>\n",
       "      <td>1340.0</td>\n",
       "      <td>4032.0</td>\n",
       "      <td>13411.0</td>\n",
       "      <td>4964.0</td>\n",
       "      <td>7590.0</td>\n",
       "      <td>2309.0</td>\n",
       "      <td>5108.0</td>\n",
       "      <td>6946.0</td>\n",
       "      <td>7036.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1316.0</td>\n",
       "      <td>1366.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>534.0</td>\n",
       "      <td>817.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5325.0</td>\n",
       "      <td>554.0</td>\n",
       "      <td>5437.0</td>\n",
       "      <td>12869.0</td>\n",
       "      <td>11326.0</td>\n",
       "      <td>8390.0</td>\n",
       "      <td>4371.0</td>\n",
       "      <td>12836.0</td>\n",
       "      <td>2871.0</td>\n",
       "      <td>2268.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>89.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>361.0</td>\n",
       "      <td>323.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>308.0</td>\n",
       "      <td>946.0</td>\n",
       "      <td>1129.0</td>\n",
       "      <td>674.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10843.0</td>\n",
       "      <td>2752.0</td>\n",
       "      <td>5456.0</td>\n",
       "      <td>8475.0</td>\n",
       "      <td>10091.0</td>\n",
       "      <td>9822.0</td>\n",
       "      <td>5626.0</td>\n",
       "      <td>9909.0</td>\n",
       "      <td>7742.0</td>\n",
       "      <td>7255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>367.0</td>\n",
       "      <td>395.0</td>\n",
       "      <td>1114.0</td>\n",
       "      <td>968.0</td>\n",
       "      <td>607.0</td>\n",
       "      <td>557.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>954.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2446.0</td>\n",
       "      <td>3844.0</td>\n",
       "      <td>3924.0</td>\n",
       "      <td>13650.0</td>\n",
       "      <td>8741.0</td>\n",
       "      <td>13681.0</td>\n",
       "      <td>4822.0</td>\n",
       "      <td>4766.0</td>\n",
       "      <td>8023.0</td>\n",
       "      <td>7885.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>988.0</td>\n",
       "      <td>889.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>1380.0</td>\n",
       "      <td>836.0</td>\n",
       "      <td>905.0</td>\n",
       "      <td>648.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>1307.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>...</td>\n",
       "      <td>11534.0</td>\n",
       "      <td>4371.0</td>\n",
       "      <td>4248.0</td>\n",
       "      <td>9874.0</td>\n",
       "      <td>9630.0</td>\n",
       "      <td>9898.0</td>\n",
       "      <td>5467.0</td>\n",
       "      <td>6585.0</td>\n",
       "      <td>1252.0</td>\n",
       "      <td>1886.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>554.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>638.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>372.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>632.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8858.0</td>\n",
       "      <td>1565.0</td>\n",
       "      <td>4859.0</td>\n",
       "      <td>11640.0</td>\n",
       "      <td>6555.0</td>\n",
       "      <td>7502.0</td>\n",
       "      <td>3013.0</td>\n",
       "      <td>3186.0</td>\n",
       "      <td>7075.0</td>\n",
       "      <td>6922.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1503.0</td>\n",
       "      <td>655.0</td>\n",
       "      <td>927.0</td>\n",
       "      <td>624.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>379.0</td>\n",
       "      <td>815.0</td>\n",
       "      <td>944.0</td>\n",
       "      <td>413.0</td>\n",
       "      <td>826.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6039.0</td>\n",
       "      <td>3058.0</td>\n",
       "      <td>3066.0</td>\n",
       "      <td>13843.0</td>\n",
       "      <td>4702.0</td>\n",
       "      <td>6947.0</td>\n",
       "      <td>3803.0</td>\n",
       "      <td>5725.0</td>\n",
       "      <td>6208.0</td>\n",
       "      <td>6108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>535.0</td>\n",
       "      <td>1321.0</td>\n",
       "      <td>331.0</td>\n",
       "      <td>893.0</td>\n",
       "      <td>425.0</td>\n",
       "      <td>648.0</td>\n",
       "      <td>1585.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>881.0</td>\n",
       "      <td>1025.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13541.0</td>\n",
       "      <td>3039.0</td>\n",
       "      <td>3873.0</td>\n",
       "      <td>14048.0</td>\n",
       "      <td>13329.0</td>\n",
       "      <td>8733.0</td>\n",
       "      <td>5793.0</td>\n",
       "      <td>4492.0</td>\n",
       "      <td>7816.0</td>\n",
       "      <td>7999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1279.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>538.0</td>\n",
       "      <td>1742.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>771.0</td>\n",
       "      <td>2098.0</td>\n",
       "      <td>1083.0</td>\n",
       "      <td>1359.0</td>\n",
       "      <td>1436.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7701.0</td>\n",
       "      <td>5947.0</td>\n",
       "      <td>7509.0</td>\n",
       "      <td>12368.0</td>\n",
       "      <td>8283.0</td>\n",
       "      <td>8230.0</td>\n",
       "      <td>7160.0</td>\n",
       "      <td>9050.0</td>\n",
       "      <td>1256.0</td>\n",
       "      <td>1095.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>717.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>474.0</td>\n",
       "      <td>1517.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>652.0</td>\n",
       "      <td>815.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3963.0</td>\n",
       "      <td>7280.0</td>\n",
       "      <td>482.0</td>\n",
       "      <td>8152.0</td>\n",
       "      <td>10842.0</td>\n",
       "      <td>13602.0</td>\n",
       "      <td>8196.0</td>\n",
       "      <td>8667.0</td>\n",
       "      <td>7133.0</td>\n",
       "      <td>7148.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1188.0</td>\n",
       "      <td>1057.0</td>\n",
       "      <td>1402.0</td>\n",
       "      <td>699.0</td>\n",
       "      <td>464.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>510.0</td>\n",
       "      <td>931.0</td>\n",
       "      <td>1243.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6598.0</td>\n",
       "      <td>7373.0</td>\n",
       "      <td>7847.0</td>\n",
       "      <td>11955.0</td>\n",
       "      <td>10590.0</td>\n",
       "      <td>8602.0</td>\n",
       "      <td>9626.0</td>\n",
       "      <td>9397.0</td>\n",
       "      <td>9254.0</td>\n",
       "      <td>7471.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>502.0</td>\n",
       "      <td>1327.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>634.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>1509.0</td>\n",
       "      <td>639.0</td>\n",
       "      <td>1722.0</td>\n",
       "      <td>694.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6385.0</td>\n",
       "      <td>6863.0</td>\n",
       "      <td>1107.0</td>\n",
       "      <td>14025.0</td>\n",
       "      <td>10583.0</td>\n",
       "      <td>11714.0</td>\n",
       "      <td>7330.0</td>\n",
       "      <td>7373.0</td>\n",
       "      <td>2815.0</td>\n",
       "      <td>2201.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>625.0</td>\n",
       "      <td>1579.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>584.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>1109.0</td>\n",
       "      <td>2497.0</td>\n",
       "      <td>851.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5060.0</td>\n",
       "      <td>12398.0</td>\n",
       "      <td>6453.0</td>\n",
       "      <td>6463.0</td>\n",
       "      <td>6437.0</td>\n",
       "      <td>6681.0</td>\n",
       "      <td>7231.0</td>\n",
       "      <td>6545.0</td>\n",
       "      <td>7628.0</td>\n",
       "      <td>6715.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1020.0</td>\n",
       "      <td>1803.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>1412.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2805.0</td>\n",
       "      <td>1123.0</td>\n",
       "      <td>457.0</td>\n",
       "      <td>519.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8438.0</td>\n",
       "      <td>13454.0</td>\n",
       "      <td>2322.0</td>\n",
       "      <td>14074.0</td>\n",
       "      <td>1422.0</td>\n",
       "      <td>1340.0</td>\n",
       "      <td>8120.0</td>\n",
       "      <td>1559.0</td>\n",
       "      <td>5584.0</td>\n",
       "      <td>5100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0currentGold  1currentGold  2currentGold  3currentGold  4currentGold  \\\n",
       "0          500.0         500.0         500.0         500.0         500.0   \n",
       "1           25.0         420.0          25.0          25.0          25.0   \n",
       "2          122.0         190.0         115.0         122.0          68.0   \n",
       "3          356.0         634.0         486.0         521.0         276.0   \n",
       "4          128.0        1475.0         128.0         750.0         485.0   \n",
       "5          453.0         212.0         454.0          57.0          80.0   \n",
       "6          803.0         630.0         751.0         439.0         214.0   \n",
       "7         1316.0        1366.0          37.0          23.0         534.0   \n",
       "8           89.0         231.0         361.0         323.0         119.0   \n",
       "9          367.0         395.0        1114.0         968.0         607.0   \n",
       "10         988.0         889.0         148.0        1380.0         836.0   \n",
       "11         554.0         241.0         638.0         177.0          70.0   \n",
       "12        1503.0         655.0         927.0         624.0         246.0   \n",
       "13         535.0        1321.0         331.0         893.0         425.0   \n",
       "14        1279.0         118.0         538.0        1742.0         195.0   \n",
       "15         717.0         640.0         800.0         194.0         474.0   \n",
       "16        1188.0        1057.0        1402.0         699.0         464.0   \n",
       "17         502.0        1327.0          65.0          21.0         634.0   \n",
       "18         625.0        1579.0         432.0         584.0         125.0   \n",
       "19        1020.0        1803.0         140.0        1412.0          65.0   \n",
       "\n",
       "    5currentGold  6currentGold  7currentGold  8currentGold  9currentGold  ...  \\\n",
       "0          500.0         500.0         500.0         500.0         500.0  ...   \n",
       "1            0.0           0.0           0.0           0.0           0.0  ...   \n",
       "2           41.0         144.0         125.0          83.0          22.0  ...   \n",
       "3          359.0         540.0         380.0         373.0         198.0  ...   \n",
       "4          816.0          99.0          89.0         648.0          94.0  ...   \n",
       "5          264.0         413.0         379.0        1050.0         228.0  ...   \n",
       "6          527.0         696.0         750.0         273.0         429.0  ...   \n",
       "7          817.0         198.0          74.0         224.0          29.0  ...   \n",
       "8          308.0         946.0        1129.0         674.0         238.0  ...   \n",
       "9          557.0          75.0          57.0         954.0         195.0  ...   \n",
       "10         905.0         648.0         329.0        1307.0         371.0  ...   \n",
       "11         215.0         286.0         372.0         207.0         632.0  ...   \n",
       "12         379.0         815.0         944.0         413.0         826.0  ...   \n",
       "13         648.0        1585.0         296.0         881.0        1025.0  ...   \n",
       "14         771.0        2098.0        1083.0        1359.0        1436.0  ...   \n",
       "15        1517.0          35.0         652.0         815.0         207.0  ...   \n",
       "16         204.0         510.0         931.0        1243.0         197.0  ...   \n",
       "17         549.0        1509.0         639.0        1722.0         694.0  ...   \n",
       "18        1109.0        2497.0         851.0          45.0         234.0  ...   \n",
       "19          13.0        2805.0        1123.0         457.0         519.0  ...   \n",
       "\n",
       "         1y       3y      4y       5y       6y       7y       8y       9y  \\\n",
       "0     361.0    471.0   649.0  14511.0  14291.0  14223.0  14401.0  14579.0   \n",
       "1     542.0   1013.0   679.0  13885.0  11463.0   8416.0   7493.0  11457.0   \n",
       "2    8042.0   2285.0  2107.0  12344.0  10103.0   7015.0   3101.0   2597.0   \n",
       "3    5011.0   3118.0  3266.0  13855.0   4991.0   7337.0   3991.0   4151.0   \n",
       "4    3263.0   2921.0  2816.0  10840.0   9761.0   9040.0   3937.0  11836.0   \n",
       "5    7169.0   1556.0  1712.0  12890.0   8567.0   8337.0   3373.0   3711.0   \n",
       "6   12602.0   1340.0  4032.0  13411.0   4964.0   7590.0   2309.0   5108.0   \n",
       "7    5325.0    554.0  5437.0  12869.0  11326.0   8390.0   4371.0  12836.0   \n",
       "8   10843.0   2752.0  5456.0   8475.0  10091.0   9822.0   5626.0   9909.0   \n",
       "9    2446.0   3844.0  3924.0  13650.0   8741.0  13681.0   4822.0   4766.0   \n",
       "10  11534.0   4371.0  4248.0   9874.0   9630.0   9898.0   5467.0   6585.0   \n",
       "11   8858.0   1565.0  4859.0  11640.0   6555.0   7502.0   3013.0   3186.0   \n",
       "12   6039.0   3058.0  3066.0  13843.0   4702.0   6947.0   3803.0   5725.0   \n",
       "13  13541.0   3039.0  3873.0  14048.0  13329.0   8733.0   5793.0   4492.0   \n",
       "14   7701.0   5947.0  7509.0  12368.0   8283.0   8230.0   7160.0   9050.0   \n",
       "15   3963.0   7280.0   482.0   8152.0  10842.0  13602.0   8196.0   8667.0   \n",
       "16   6598.0   7373.0  7847.0  11955.0  10590.0   8602.0   9626.0   9397.0   \n",
       "17   6385.0   6863.0  1107.0  14025.0  10583.0  11714.0   7330.0   7373.0   \n",
       "18   5060.0  12398.0  6453.0   6463.0   6437.0   6681.0   7231.0   6545.0   \n",
       "19   8438.0  13454.0  2322.0  14074.0   1422.0   1340.0   8120.0   1559.0   \n",
       "\n",
       "    player_x  player_y  \n",
       "0      351.0     293.0  \n",
       "1     6348.0    9817.0  \n",
       "2     6598.0    6970.0  \n",
       "3     7235.0    6689.0  \n",
       "4     7473.0    7689.0  \n",
       "5     8044.0    7598.0  \n",
       "6     6946.0    7036.0  \n",
       "7     2871.0    2268.0  \n",
       "8     7742.0    7255.0  \n",
       "9     8023.0    7885.0  \n",
       "10    1252.0    1886.0  \n",
       "11    7075.0    6922.0  \n",
       "12    6208.0    6108.0  \n",
       "13    7816.0    7999.0  \n",
       "14    1256.0    1095.0  \n",
       "15    7133.0    7148.0  \n",
       "16    9254.0    7471.0  \n",
       "17    2815.0    2201.0  \n",
       "18    7628.0    6715.0  \n",
       "19    5584.0    5100.0  \n",
       "\n",
       "[20 rows x 60 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "    def __init__(self, input_width, label_width, shift,\n",
    "                 full_df=test0, val_df = test1,\n",
    "                 dfdi=dfs_dict,\n",
    "                 #train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "                 label_columns=None):\n",
    "        # Store the raw data.\n",
    "        #self.train_df = train_df\n",
    "        #self.val_df = val_df\n",
    "        #self.test_df = test_df\n",
    "        self.full_df = full_df\n",
    "        self.val_df = val_df\n",
    "        self.dfdi = dfdi\n",
    "        self.trainli = None\n",
    "        self.valli = None\n",
    "        self.testli = None\n",
    "\n",
    "        # Work out the label column indices.\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {name: i for i, name in\n",
    "                                                                        enumerate(label_columns)}\n",
    "        self.column_indices = {name: i for i, name in\n",
    "                                                     enumerate(full_df.columns)}\n",
    "\n",
    "        # Work out the window parameters.\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "\n",
    "        self.total_window_size = input_width + shift\n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "                f'Total window size: {self.total_window_size}',\n",
    "                f'Input indices: {self.input_indices}',\n",
    "                f'Label indices: {self.label_indices}',\n",
    "                f'Label column name(s): {self.label_columns}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_window(self, features):\n",
    "    inputs = features[:, self.input_slice, :]\n",
    "    labels = features[:, self.labels_slice, :]\n",
    "    if self.label_columns is not None:\n",
    "        labels = tf.stack(\n",
    "                [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "                axis=-1)\n",
    "    \n",
    "\n",
    "    # Slicing doesn't preserve static shape information, so set the shapes\n",
    "    # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "    inputs.set_shape([None, self.input_width, None])\n",
    "    labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "    return inputs, labels\n",
    "\n",
    "WindowGenerator.split_window = split_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(self, model=None, plot_col=\"player_x\", max_subplots=3):\n",
    "    inputs, labels = self.example\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plot_col_index = self.column_indices[plot_col]\n",
    "    print(plot_col_index, type(plot_col_index))\n",
    "    max_n = min(max_subplots, len(inputs))\n",
    "    for n in range(max_n):\n",
    "        plt.subplot(3, 1, n+1)\n",
    "        plt.ylabel(f'{plot_col} [normed]')\n",
    "        plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n",
    "                         label='Inputs', marker='.', zorder=-10)\n",
    "\n",
    "        if self.label_columns:\n",
    "            print(type(self.label_columns_indices))\n",
    "            label_col_index = self.label_columns_indices.get(plot_col, None)\n",
    "            print(label_col_index)\n",
    "        else:\n",
    "            label_col_index = plot_col_index\n",
    "\n",
    "        if label_col_index is None:\n",
    "            continue\n",
    "\n",
    "        print(self.label_indices)\n",
    "        plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
    "                                edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "        if model is not None:\n",
    "            predictions = model(inputs)\n",
    "            plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n",
    "                                    marker='X', edgecolors='k', label='Predictions',\n",
    "                                    c='#ff7f0e', s=64)\n",
    "\n",
    "        if n == 0:\n",
    "            plt.legend()\n",
    "\n",
    "    plt.xlabel('Time [h]')\n",
    "\n",
    "WindowGenerator.plot = plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(self, data, list_me=False):\n",
    "    data = np.array(data, dtype=np.float32)\n",
    "    ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "            data=data,\n",
    "            targets=None,\n",
    "            sequence_length=self.total_window_size,\n",
    "            sequence_stride=1,\n",
    "            shuffle=True,\n",
    "            batch_size=32,)\n",
    "\n",
    "    ds = ds.map(self.split_window)\n",
    "    if list_me:\n",
    "        return list(ds)\n",
    "    return ds\n",
    "\n",
    "WindowGenerator.make_dataset = make_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_split_from_dfdi(self,splits=(.8,.9),minmax=(0,20)):\n",
    "    # minmax is start and end time of window being sampled from.\n",
    "    # splits is where the data is being split for train val test split\n",
    "    if self.dfdi is None:\n",
    "        print(\"no dfdi attached\")\n",
    "        raise KeyError(\"no dfdi attached\")\n",
    "    gameids = list(self.dfdi.keys())\n",
    "    gameids.sort()\n",
    "    train_ids = gameids[0:int(len(gameids)*splits[0])]\n",
    "    val_ids = gameids[int(len(gameids)*splits[0]):int(len(gameids)*splits[1])]\n",
    "    test_ids = gameids[int(len(gameids)*splits[1]):]\n",
    "    if self.trainli is None:\n",
    "        trainli = build_batch(self, self.dfdi, train_ids, minmax)\n",
    "        train_feats = tf.concat([batch[0] for batch in trainli], axis=0)\n",
    "        train_labels = tf.concat([batch[1] for batch in trainli], axis=0)\n",
    "        \n",
    "    if self.valli is None:\n",
    "        valli = build_batch(self, self.dfdi, val_ids, minmax)\n",
    "        val_feats = tf.concat([batch[0] for batch in valli], axis=0)\n",
    "        val_labels = tf.concat([batch[1] for batch in valli], axis=0)\n",
    "            \n",
    "    if self.testli is None:\n",
    "        testli = build_batch(self, self.dfdi, test_ids, minmax)\n",
    "        test_feats = tf.concat([batch[0] for batch in testli], axis=0)\n",
    "        test_labels = tf.concat([batch[1] for batch in testli], axis=0)\n",
    "            \n",
    "        \n",
    "    print(train_feats.shape, train_labels.shape)\n",
    "    self.train = (train_feats, train_labels)\n",
    "    self.val = (val_feats, val_labels)\n",
    "    self.test = (test_feats, test_labels)\n",
    "\n",
    "WindowGenerator.make_split_from_dfdi = make_split_from_dfdi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@property\n",
    "def example(self):\n",
    "    \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "    result = getattr(self, '_example', None)\n",
    "    if result is None:\n",
    "        # No example batch was found, so get one from the `.train` dataset\n",
    "        result = next(iter(self.train))\n",
    "        # And cache it for next time\n",
    "        self._example = result\n",
    "    return result\n",
    "\n",
    "WindowGenerator.example = example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_batch(self, dfdi, id_li, minmax):\n",
    "    dataset_li = []\n",
    "    for match_id in id_li:\n",
    "        df = dfdi[match_id]\n",
    "        df = df.drop([\"timestamp\"], axis=1)\n",
    "        df = df.iloc[minmax[0]:minmax[1]]\n",
    "        dataset_li = dataset_li + make_dataset(self, data=df, list_me=True)\n",
    "    return dataset_li\n",
    "\n",
    "WindowGenerator.build_batch = build_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['0currentGold', '1currentGold', '2currentGold', '3currentGold',\n",
       "       '4currentGold', '5currentGold', '6currentGold', '7currentGold',\n",
       "       '8currentGold', '9currentGold', '0totalGold', '1totalGold',\n",
       "       '2totalGold', '3totalGold', '4totalGold', '5totalGold', '6totalGold',\n",
       "       '7totalGold', '8totalGold', '9totalGold', '0level', '1level', '2level',\n",
       "       '3level', '4level', '5level', '6level', '7level', '8level', '9level',\n",
       "       '0jungleMinionsKilled', '1jungleMinionsKilled', '2jungleMinionsKilled',\n",
       "       '3jungleMinionsKilled', '4jungleMinionsKilled', '5jungleMinionsKilled',\n",
       "       '6jungleMinionsKilled', '7jungleMinionsKilled', '8jungleMinionsKilled',\n",
       "       '9jungleMinionsKilled', '0x', '1x', '3x', '4x', '5x', '6x', '7x', '8x',\n",
       "       '9x', '0y', '1y', '3y', '4y', '5y', '6y', '7y', '8y', '9y', 'player_x',\n",
       "       'player_y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test0.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Total window size: 7\n",
       "Input indices: [0 1 2 3 4]\n",
       "Label indices: [5 6]\n",
       "Label column name(s): ['player_x', 'player_y']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2 = WindowGenerator(input_width=5, label_width=2, shift=2,\n",
    "                     label_columns=[\"player_x\",\"player_y\"])\n",
    "w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline(tf.keras.Model):\n",
    "    def __init__(self, label_index=None):\n",
    "        super().__init__()\n",
    "        self.label_index = label_index\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if self.label_index is None:\n",
    "            return inputs\n",
    "        result = inputs[:, :, self.label_index]\n",
    "        return result[:, :, tf.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Total window size: 2\n",
       "Input indices: [0]\n",
       "Label indices: [1]\n",
       "Label column name(s): ['player_x']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_step_window = WindowGenerator(\n",
    "    input_width=1, label_width=1, shift=1,\n",
    "    label_columns=[\"player_x\"])#,\"player_y\"\n",
    "single_step_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1824, 1, 60) (1824, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "single_step_window.make_split_from_dfdi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hold = single_step_window.train# = tf.data.Dataset.from_tensor_slices([train_feats, train_labels])\n",
    "val_hold = single_step_window.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = tf.data.Dataset.from_tensor_slices(train_hold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = Baseline()\n",
    "baseline.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                 metrics=[tf.metrics.MeanAbsoluteError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 54875640.0000 - mean_absolute_error: 6321.6372 - val_loss: 54875624.0000 - val_mean_absolute_error: 6321.6348\n",
      "Epoch 2/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 54875632.0000 - mean_absolute_error: 6321.6348 - val_loss: 54875624.0000 - val_mean_absolute_error: 6321.6348\n",
      "Epoch 3/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 54875624.0000 - mean_absolute_error: 6321.6353 - val_loss: 54875624.0000 - val_mean_absolute_error: 6321.6348\n",
      "Epoch 4/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 54875632.0000 - mean_absolute_error: 6321.6372 - val_loss: 54875624.0000 - val_mean_absolute_error: 6321.6348\n",
      "Epoch 5/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 54875620.0000 - mean_absolute_error: 6321.6353 - val_loss: 54875624.0000 - val_mean_absolute_error: 6321.6348\n",
      "Epoch 6/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 54875612.0000 - mean_absolute_error: 6321.6357 - val_loss: 54875624.0000 - val_mean_absolute_error: 6321.6348\n",
      "Epoch 7/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 54875620.0000 - mean_absolute_error: 6321.6353 - val_loss: 54875624.0000 - val_mean_absolute_error: 6321.6348\n",
      "Epoch 8/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 54875624.0000 - mean_absolute_error: 6321.6348 - val_loss: 54875624.0000 - val_mean_absolute_error: 6321.6348\n",
      "Epoch 9/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 54875620.0000 - mean_absolute_error: 6321.6367 - val_loss: 54875624.0000 - val_mean_absolute_error: 6321.6348\n",
      "Epoch 10/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 54875624.0000 - mean_absolute_error: 6321.6353 - val_loss: 54875624.0000 - val_mean_absolute_error: 6321.6348\n",
      "Epoch 11/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 54875632.0000 - mean_absolute_error: 6321.6367 - val_loss: 54875624.0000 - val_mean_absolute_error: 6321.6348\n",
      "Epoch 12/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 54875632.0000 - mean_absolute_error: 6321.6353 - val_loss: 54875624.0000 - val_mean_absolute_error: 6321.6348\n",
      "Epoch 13/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 54875620.0000 - mean_absolute_error: 6321.6353 - val_loss: 54875624.0000 - val_mean_absolute_error: 6321.6348\n",
      "Epoch 14/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 54875636.0000 - mean_absolute_error: 6321.6353 - val_loss: 54875624.0000 - val_mean_absolute_error: 6321.6348\n",
      "Epoch 15/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 54875640.0000 - mean_absolute_error: 6321.6367 - val_loss: 54875624.0000 - val_mean_absolute_error: 6321.6348\n",
      "Epoch 16/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 54875624.0000 - mean_absolute_error: 6321.6353 - val_loss: 54875624.0000 - val_mean_absolute_error: 6321.6348\n",
      "Epoch 17/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 54875624.0000 - mean_absolute_error: 6321.6348 - val_loss: 54875624.0000 - val_mean_absolute_error: 6321.6348\n",
      "Epoch 18/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 54875620.0000 - mean_absolute_error: 6321.6353 - val_loss: 54875624.0000 - val_mean_absolute_error: 6321.6348\n",
      "Epoch 19/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 54875620.0000 - mean_absolute_error: 6321.6353 - val_loss: 54875624.0000 - val_mean_absolute_error: 6321.6348\n",
      "Epoch 20/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 54875620.0000 - mean_absolute_error: 6321.6348 - val_loss: 54875624.0000 - val_mean_absolute_error: 6321.6348\n",
      "Epoch 21/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 54875632.0000 - mean_absolute_error: 6321.6367 - val_loss: 54875624.0000 - val_mean_absolute_error: 6321.6348\n",
      "Epoch 22/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 54875640.0000 - mean_absolute_error: 6321.6353 - val_loss: 54875624.0000 - val_mean_absolute_error: 6321.6348\n",
      "Epoch 23/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 54875620.0000 - mean_absolute_error: 6321.6353 - val_loss: 54875624.0000 - val_mean_absolute_error: 6321.6348\n",
      "Epoch 24/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 54875636.0000 - mean_absolute_error: 6321.6353 - val_loss: 54875624.0000 - val_mean_absolute_error: 6321.6348\n",
      "Epoch 25/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 54875636.0000 - mean_absolute_error: 6321.6367 - val_loss: 54875624.0000 - val_mean_absolute_error: 6321.6348\n",
      "Epoch 26/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 54875632.0000 - mean_absolute_error: 6321.6367 - val_loss: 54875624.0000 - val_mean_absolute_error: 6321.6348\n",
      "Epoch 27/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 54875620.0000 - mean_absolute_error: 6321.6357 - val_loss: 54875624.0000 - val_mean_absolute_error: 6321.6348\n",
      "Epoch 28/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 54875620.0000 - mean_absolute_error: 6321.6357 - val_loss: 54875624.0000 - val_mean_absolute_error: 6321.6348\n",
      "Epoch 29/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 54875624.0000 - mean_absolute_error: 6321.6357 - val_loss: 54875624.0000 - val_mean_absolute_error: 6321.6348\n",
      "Epoch 30/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 54875624.0000 - mean_absolute_error: 6321.6353 - val_loss: 54875624.0000 - val_mean_absolute_error: 6321.6348\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fab2bb21290>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline.fit(x=train_hold[0],\n",
    "           y=train_hold[1],\n",
    "           batch_size=32,\n",
    "           epochs=30,\n",
    "           verbose=1,\n",
    "           validation_data=val_hold\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                optimizer=tf.optimizers.Adam(),\n",
    "                metrics=[tf.metrics.MeanAbsoluteError()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 149434352.0000 - mean_absolute_error: 11251.7129 - val_loss: 59940728.0000 - val_mean_absolute_error: 6862.8628\n",
      "Epoch 2/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 31957418.0000 - mean_absolute_error: 4697.3525 - val_loss: 17814664.0000 - val_mean_absolute_error: 3445.5137\n",
      "Epoch 3/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 15700956.0000 - mean_absolute_error: 3223.5222 - val_loss: 14867258.0000 - val_mean_absolute_error: 3124.3591\n",
      "Epoch 4/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 14730993.0000 - mean_absolute_error: 3106.9719 - val_loss: 14606076.0000 - val_mean_absolute_error: 3091.3533\n",
      "Epoch 5/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 14507820.0000 - mean_absolute_error: 3079.6697 - val_loss: 14384568.0000 - val_mean_absolute_error: 3066.8416\n",
      "Epoch 6/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 14290408.0000 - mean_absolute_error: 3056.7319 - val_loss: 14159277.0000 - val_mean_absolute_error: 3040.5361\n",
      "Epoch 7/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 14072305.0000 - mean_absolute_error: 3033.6006 - val_loss: 13931107.0000 - val_mean_absolute_error: 3014.7563\n",
      "Epoch 8/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 13831799.0000 - mean_absolute_error: 3003.8562 - val_loss: 13701793.0000 - val_mean_absolute_error: 2990.1130\n",
      "Epoch 9/30\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 13601811.0000 - mean_absolute_error: 2980.8237 - val_loss: 13472960.0000 - val_mean_absolute_error: 2965.2747\n",
      "Epoch 10/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 13383550.0000 - mean_absolute_error: 2955.3611 - val_loss: 13258161.0000 - val_mean_absolute_error: 2944.6191\n",
      "Epoch 11/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 13168783.0000 - mean_absolute_error: 2932.5339 - val_loss: 13043520.0000 - val_mean_absolute_error: 2921.4224\n",
      "Epoch 12/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 12955464.0000 - mean_absolute_error: 2911.4534 - val_loss: 12838400.0000 - val_mean_absolute_error: 2891.7612\n",
      "Epoch 13/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 12753938.0000 - mean_absolute_error: 2884.5378 - val_loss: 12638584.0000 - val_mean_absolute_error: 2871.0913\n",
      "Epoch 14/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 12553795.0000 - mean_absolute_error: 2858.1992 - val_loss: 12444593.0000 - val_mean_absolute_error: 2850.8088\n",
      "Epoch 15/30\n",
      "57/57 [==============================] - 0s 3ms/step - loss: 12383205.0000 - mean_absolute_error: 2843.0979 - val_loss: 12259641.0000 - val_mean_absolute_error: 2826.9895\n",
      "Epoch 16/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 12208322.0000 - mean_absolute_error: 2820.0920 - val_loss: 12106465.0000 - val_mean_absolute_error: 2811.8054\n",
      "Epoch 17/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 12045504.0000 - mean_absolute_error: 2802.0735 - val_loss: 11940461.0000 - val_mean_absolute_error: 2790.3728\n",
      "Epoch 18/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 11869220.0000 - mean_absolute_error: 2780.6897 - val_loss: 11778916.0000 - val_mean_absolute_error: 2767.4324\n",
      "Epoch 19/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 11730603.0000 - mean_absolute_error: 2762.3347 - val_loss: 11636516.0000 - val_mean_absolute_error: 2748.1653\n",
      "Epoch 20/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 11585134.0000 - mean_absolute_error: 2740.5044 - val_loss: 11506915.0000 - val_mean_absolute_error: 2732.3777\n",
      "Epoch 21/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 11459591.0000 - mean_absolute_error: 2726.7822 - val_loss: 11379367.0000 - val_mean_absolute_error: 2712.6636\n",
      "Epoch 22/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 11354298.0000 - mean_absolute_error: 2709.0400 - val_loss: 11277008.0000 - val_mean_absolute_error: 2696.8042\n",
      "Epoch 23/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 11233741.0000 - mean_absolute_error: 2692.4917 - val_loss: 11153482.0000 - val_mean_absolute_error: 2681.9707\n",
      "Epoch 24/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 11118609.0000 - mean_absolute_error: 2676.8342 - val_loss: 11057022.0000 - val_mean_absolute_error: 2667.2256\n",
      "Epoch 25/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 11041786.0000 - mean_absolute_error: 2665.4995 - val_loss: 10957533.0000 - val_mean_absolute_error: 2654.8372\n",
      "Epoch 26/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 10940291.0000 - mean_absolute_error: 2652.0115 - val_loss: 10869626.0000 - val_mean_absolute_error: 2642.1855\n",
      "Epoch 27/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 10841058.0000 - mean_absolute_error: 2637.3748 - val_loss: 10791990.0000 - val_mean_absolute_error: 2627.7959\n",
      "Epoch 28/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 10813165.0000 - mean_absolute_error: 2632.4487 - val_loss: 10756806.0000 - val_mean_absolute_error: 2629.0261\n",
      "Epoch 29/30\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 10680821.0000 - mean_absolute_error: 2614.9053 - val_loss: 10675668.0000 - val_mean_absolute_error: 2606.1848\n",
      "Epoch 30/30\n",
      "57/57 [==============================] - 0s 2ms/step - loss: 10640618.0000 - mean_absolute_error: 2601.5425 - val_loss: 10578667.0000 - val_mean_absolute_error: 2598.8259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fab29def210>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear.fit(x=train_hold[0],\n",
    "           y=train_hold[1],\n",
    "           batch_size=32,\n",
    "           epochs=30,\n",
    "           verbose=1,\n",
    "           validation_data=val_hold\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hold = single_step_window.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_test_pred = linear.predict(x=test_hold[0])\n",
    "baseline_test_pred = baseline.predict(x=test_hold[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 1ms/step - loss: 11266763.0000 - mean_absolute_error: 2702.7046\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 67996880.0000 - mean_absolute_error: 7220.9014WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0011s vs `on_test_batch_end` time: 0.0019s). Check your callbacks.\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 61573616.0000 - mean_absolute_error: 6712.8027\n",
      "baseline mae error: 6712.802734375\n",
      "linear mae error: 2702.70458984375\n"
     ]
    }
   ],
   "source": [
    "linear_error = linear.evaluate(test_hold[0], test_hold[1])#, batch_size=128)\n",
    "baseline_error = baseline.evaluate(test_hold[0], test_hold[1])\n",
    "print(\"baseline mae error:\", baseline_error[1])\n",
    "print(\"linear mae error:\", linear_error[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Total window size: 11\n",
       "Input indices: [0 1 2 3 4 5 6 7 8 9]\n",
       "Label indices: [ 1  2  3  4  5  6  7  8  9 10]\n",
       "Label column name(s): ['player_x']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wide_window = WindowGenerator(\n",
    "    input_width=10, label_width=10, shift=1,\n",
    "    label_columns=[\"player_x\"])\n",
    "\n",
    "wide_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WindowGenerator' object has no attribute 'full'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-b956bdeb31fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msingle_step_window\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-3e5a4fc4fb72>\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, model, plot_col, max_subplots)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"player_x\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_subplots\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mplot_col_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplot_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot_col_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot_col_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-c2072d29be0e>\u001b[0m in \u001b[0;36mexample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# No example batch was found, so get one from the `.train` dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;31m# And cache it for next time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_example\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'WindowGenerator' object has no attribute 'full'"
     ]
    }
   ],
   "source": [
    "single_step_window.plot(linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
